
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>PubSummarizer - ICLR 2024 Oral Papers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <style>
        .filter-controls {
            margin-bottom: 20px;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 8px;
        }
        .pagination-controls {
            margin: 20px 0;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 8px;
        }
        @media (max-width: 991px) {
            .search-box {
                margin-bottom: 15px;
            }
        }
    </style>
</head>
<body>
    <div class="container py-4">
        <h1 class="mb-4">ICLR 2024 Oral Papers</h1>
        <p class="text-muted"><em>Generated by <a href="https://github.com/Insights-Ac/PaperBriefing">PaperBriefing</a></em></p>
        
        <div class="filter-controls">
            <div class="d-flex flex-md-row flex-column gap-1">
                <div class="flex-grow-1">
                    <input type="text" class="form-control form-control-sm search-box" id="searchInput" 
                           placeholder="Search in titles, topics, TL;DR, and summaries...">
                </div>
                <div class="btn-group btn-group-sm" role="group" aria-label="Section toggles">
                    <input type="checkbox" class="btn-check" id="showTopics" checked autocomplete="off">
                    <label class="btn btn-outline-primary" for="showTopics">Topics</label>

                    <input type="checkbox" class="btn-check" id="showTldr" checked autocomplete="off">
                    <label class="btn btn-outline-primary" for="showTldr">TL;DR</label>

                    <input type="checkbox" class="btn-check" id="showSummary" checked autocomplete="off">
                    <label class="btn btn-outline-primary" for="showSummary">Summary</label>
                </div>
            </div>
        </div>

        <div class="pagination-controls">
            <div class="d-flex flex-wrap justify-content-between align-items-center gap-1">
                <div class="d-flex align-items-center gap-2">
                    <label for="itemsPerPage">Items per page:</label>
                    <select class="form-select form-select-sm" id="itemsPerPage" style="width: auto;">
                        <option value="6">6</option>
                        <option value="12" selected>12</option>
                        <option value="24">24</option>
                        <option value="48">48</option>
                    </select>
                </div>
                <nav aria-label="Page navigation">
                    <ul class="pagination pagination-sm mb-0" id="pagination"></ul>
                </nav>
            </div>
        </div>

        <div id="papers-container" class="row" data-masonry='{"percentPosition": true }'></div>
    </div>

    <script>
        // Store papers data and pagination state
        const papersData = [{"title": "\"What Data Benefits My Classifier?\" Enhancing Model Performance and Interpretability through Influence-Based Data Selection", "pdf_url": "https://openreview.net/attachment?id=HE9eUQlAvo&name=pdf", "topics": ["influence functions", "data selection", "model performance", "fairness", "robustness"], "tldr": "This paper introduces an influence-based data selection approach to enhance the performance and interpretability of classifiers in terms of utility, fairness, and robustness, validated across various application scenarios.", "summary": "The paper addresses the critical yet often overlooked issue of data selection to improve classification model performance by utilizing influence functions to evaluate the impact of training samples on utility, fairness, and robustness. It proposes influence estimation models, particularly tree-based methods, to interpret which features contribute positively or negatively to model outcomes, and presents data trimming strategies to enhance performance within specific budgets. Through extensive experiments on both synthetic and real-world datasets, the authors demonstrate that their approach not only improves accuracy but also enhances fairness and robustness across challenging scenarios, such as distribution shifts and adversarial attacks, underscoring its practical implications for machine learning applications."}, {"title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis", "pdf_url": "https://openreview.net/attachment?id=9JQtrumvg8&name=pdf", "topics": ["web automation", "large language models", "HTML understanding", "program synthesis", "self-experience supervision"], "tldr": "This paper presents WebAgent, an LLM-driven web automation system that improves task success on real websites using HTML-T5 for planning and summarization and Flan-U-PaLM for code generation, achieving over 50% higher success rates compared to previous methods.", "summary": "The paper introduces WebAgent, a novel autonomous agent designed to complete tasks on real websites by employing large language models (LLMs) with specialized capabilities. It addresses challenges in web automation, such as long HTML documents and open-ended action spaces, by integrating HTML-T5 for planning and summarization and the Flan-U-PaLM model for generating executable Python programs. Through self-experience supervision, which leverages data generated from scripted interactions, WebAgent significantly enhances task completion success rates by over 50% on various websites, outperforming existing methods. The findings underscore the importance of domain-specific LLMs in improving web automation performance, with implications for advancing autonomous web agents in practical applications."}, {"title": "Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks", "pdf_url": "https://openreview.net/attachment?id=BV1PHbTJzd&name=pdf", "topics": ["Self-Repellent Random Walks", "Distributed Stochastic Optimization", "Stochastic Approximation", "Convergence Analysis", "Asymptotic Covariance"], "tldr": "This paper introduces the Self-Repellent Random Walk (SRRW) to enhance distributed stochastic optimization algorithms, demonstrating superior asymptotic convergence properties and lower variance compared to traditional Markovian approaches.", "summary": "The study investigates a novel family of distributed stochastic optimization algorithms using Self-Repellent Random Walk (SRRW), which employs a non-linear Markov chain to improve the sampling efficiency of gradients in decentralized learning settings. By replacing conventional Markov chains with the SRRW, parameterized by a scalar that controls self-repellence, the authors derive a generalized algorithm termed SA-SRRW. The paper proves that the optimization errors of SA-SRRW converge to zero almost surely and establishes a central limit theorem, revealing that the asymptotic covariance matrices of this method exhibit decreased variance compared to traditional approaches, specifically achieving a rate of O(1/2). Empirical evaluations affirm the theoretical predictions, highlighting the practical benefits of using SRRW in real-world distributed optimization tasks, thus advocating for its broader adoption in stochastic optimization contexts."}, {"title": "Amortizing intractable inference in large language models", "pdf_url": "https://openreview.net/attachment?id=Ouj6p4ca60&name=pdf", "topics": ["GFlowNets", "Amortized Inference", "Large Language Models", "Chain-of-Thought Reasoning", "Intractable Posterior Sampling"], "tldr": "The paper introduces a novel approach, leveraging GFlowNets for amortized Bayesian inference, that enables large language models to sample from intractable posteriors effectively, enhancing their performance in diverse tasks.", "summary": "This paper addresses the limitations of autoregressive large language models (LLMs) in sampling from intractable posterior distributions crucial for tasks like sequence continuation and reasoning. By fine-tuning LLMs using GFlowNets\u2014diversity-seeking reinforcement learning algorithms\u2014the authors propose an innovative amortized inference method that allows LLMs to sample from complex distributions effectively. Empirical results demonstrate significant improvements in both sample diversity and task performance across various applications, including story infilling, subjectivity classification, and arithmetic reasoning. The findings suggest that GFlowNet fine-tuning not only provides better fidelity and diversity trade-offs but also improves sample efficiency and generalization in scenarios with limited data."}, {"title": "An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment", "pdf_url": "https://openreview.net/attachment?id=mE52zURNGc&name=pdf", "topics": ["Direct image alignment", "Gauss-Newton loss", "camera pose estimation", "self-supervised learning", "feature matching."], "tldr": "This paper presents an analytical solution to the Gauss-Newton loss for direct image alignment, enhancing robustness to pose initialization and achieving high alignment accuracy with self-supervised feature descriptors.", "summary": "The paper addresses the challenges of direct image alignment, particularly the reliance on initial pose accuracy for effective camera pose estimation. The authors derive a closed-form solution to the Gauss-Newton loss, allowing for dynamic adjustment of the convergence basin during optimization, which improves robustness to imprecise initializations. They demonstrate that their approach, which leverages self-supervised feature descriptors, achieves competitive performance compared to state-of-the-art end-to-end supervised methods across various datasets. The findings highlight the limitations of traditional end-to-end learning frameworks that utilize the Gauss-Newton loss, suggesting a deep connection between direct image alignment and feature matching, with implications for future research in robust feature representation."}, {"title": "Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization", "pdf_url": "https://openreview.net/attachment?id=cc8h3I3V4E&name=pdf", "topics": ["Nash Equilibrium", "Stochastic Optimization", "Monte Carlo Estimation", "Normal-Form Games", "Machine Learning"], "tldr": "This paper introduces a novel loss function for approximating Nash equilibria in normal-form games that enables unbiased Monte Carlo estimation and leverages stochastic optimization techniques, providing efficient algorithms with promising empirical performance.", "summary": "The authors propose a new loss function aimed at approximating Nash equilibria in normal-form games, which is designed for unbiased Monte Carlo estimation. This innovative approach reformulates the problem as a stochastic non-convex optimization task, allowing the application of powerful optimization methods like stochastic gradient descent (SGD). The paper includes rigorous theoretical analysis demonstrating the effectiveness of the proposed loss function and its favorable properties, alongside experimental results showing that SGD can outperform existing state-of-the-art methods in approximating Nash equilibria in larger games. The findings suggest that this new methodology can significantly advance computational game theory and its applications in multi-agent systems."}, {"title": "ASID: Active Exploration for System Identification in Robotic Manipulation", "pdf_url": "https://openreview.net/attachment?id=jNR6s6OSBT&name=pdf", "topics": ["active exploration", "system identification", "robotic manipulation", "simulation-to-reality transfer", "reinforcement learning"], "tldr": "The paper presents ASID, a novel framework that employs active exploration for system identification in robotic manipulation, allowing effective sim-to-real transfer with minimal real-world data.", "summary": "This paper introduces ASID (Active Exploration for System IDentification), a learning system designed to enhance robotic manipulation by autonomously refining simulation models through targeted exploration in the real world. The approach involves three main steps: performing exploration to gather informative data, updating simulator parameters based on real-world interactions, and training effective control policies in the refined simulation for zero-shot transfer to real-world tasks. Experimental results demonstrate that ASID effectively identifies physical parameters across various challenging tasks, requiring only a small amount of real-world data, thus bridging the sim-to-real gap commonly faced in robotics. The findings underscore the importance of carefully directed exploration in optimizing performance outcomes and practical applicability in real environments."}, {"title": "Batched Low-Rank Adaptation of Foundation Models", "pdf_url": "https://openreview.net/attachment?id=w4abltTZ2f&name=pdf", "topics": ["Low-Rank Adaptation", "FLORA", "Foundation Models", "Batching", "Throughput"], "tldr": "This paper introduces Fast Low-Rank Adaptation (FLORA), a framework that enhances the batching capabilities of Low-Rank Adaptation (LORA) for foundation models, improving throughput and reducing latency without sacrificing accuracy.", "summary": "The paper presents Fast Low-Rank Adaptation (FLORA), an extension of the Low-Rank Adaptation (LORA) method, designed to overcome the limitations of LORA in real-time serving scenarios by enabling each example in a minibatch to utilize unique low-rank adaptation weights. This modification allows for efficient batching of diverse user requests, significantly improving throughput and reducing latency compared to traditional LORA. Empirical evaluations on multilingual code generation and speech recognition tasks demonstrate that FLORA maintains LORA's accuracy while achieving up to 2X throughput improvements and halving latency, underscoring its practicality for serving large-scale foundation models in diverse applications."}, {"title": "Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness", "pdf_url": "https://openreview.net/attachment?id=HSKaGOi7Ar&name=pdf", "topics": ["expressiveness", "Graph Neural Networks", "homomorphism", "Weisfeiler-Lehman", "subgraph counting"], "tldr": "This paper introduces a novel quantitative framework based on homomorphism expressivity to assess and compare the expressive power of various Graph Neural Network (GNN) architectures.", "summary": "The paper addresses the limitations of the Weisfeiler-Lehman hierarchy in evaluating the expressiveness of Graph Neural Networks (GNNs) by proposing a new framework centered on homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. By characterizing the expressivity of several GNN classes\u2014specifically, MPNN, Subgraph GNN, Local GNN, and Folklore-type GNN\u2014the authors demonstrate how this measure allows for complete and practical comparisons of different architectures. Through extensive theoretical analysis and empirical experiments, they establish a hierarchy of expressiveness among these GNNs, reveal links to subgraph counting capabilities, and show that theoretical expressivity correlates with practical performance in various tasks, paving the way for more effective GNN designs."}, {"title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs", "pdf_url": "https://openreview.net/attachment?id=7Ttk3RzDeu&name=pdf", "topics": ["book-length summarization", "coherence errors", "LLM evaluation", "automatic metrics", "human annotations"], "tldr": "This paper introduces BOOOOK SCORE, a novel automatic metric for evaluating coherence in book-length summaries generated by large language models (LLMs), and conducts a systematic exploration of summarization methods.", "summary": "The paper addresses the challenges of summarizing book-length documents using large language models (LLMs), specifically investigating coherence errors in generated summaries through human evaluations and an automatic metric named BOOOOK SCORE. It categorizes eight types of coherence errors observed in LLM-generated summaries of recently published books, employing two summarization strategies: hierarchical merging and incremental updating. The findings reveal differences in coherence between LLMs like GPT-4, Claude 2, and others, with hierarchical merging generally producing more coherent but less detailed summaries than incremental updating. BOOOOK SCORE, validated against human annotations, provides a scalable evaluation method and facilitates further research into the summarization capabilities of various LLMs."}, {"title": "Cameras as Rays: Pose Estimation via Ray Diffusion", "pdf_url": "https://openreview.net/attachment?id=EanCFCwAjM&name=pdf", "topics": ["pose estimation", "ray representation", "camera extrinsics", "denoising diffusion", "sparse views"], "tldr": "This paper introduces a distributed ray representation for camera pose estimation, utilizing regression and denoising diffusion methods to achieve state-of-the-art performance on the CO3D dataset.", "summary": "The paper presents a novel approach to camera pose estimation that addresses the challenges posed by sparse input images (fewer than 10). Instead of predicting global camera parameters like rotation and translation, the authors propose a distributed representation where each camera is treated as a bundle of rays associated with image patches. This representation enhances the coupling with spatial image features and allows for improved pose accuracy. The authors develop a regression-based model that maps image patches to rays and extend this method using a denoising diffusion model to account for uncertainties in sparse-view scenarios. The results demonstrate that both the regression and diffusion models not only surpass existing state-of-the-art methods but also generalize to unseen object categories and real-world captures, thereby providing a promising direction for future research in 3D reconstruction."}, {"title": "Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning", "pdf_url": "https://openreview.net/attachment?id=Fk5IzauJ7F&name=pdf", "topics": ["candidate label pruning", "partial-label learning", "data-centric approach", "deep learning", "representation quality"], "tldr": "This paper introduces a novel approach called candidate label set pruning (CLSP) for partial-label learning that effectively filters out potential false candidate labels to enhance the performance of deep learning models.", "summary": "The paper presents a new task known as candidate label set pruning (CLSP), which aims to improve the performance of deep partial-label learning (PLL) by reducing the size of candidate label sets associated with training instances. It proposes a training-free method that utilizes the inconsistency between the representation space of instances and their candidate label sets. The method identifies and eliminates high-probability false candidate labels by analyzing the labels from the k-nearest neighbors of an instance. Theoretical analyses are provided to establish an upper bound on the pruning error and to examine how representation quality affects the proposed method. Extensive experiments conducted on both synthetic and real-world datasets demonstrate significant improvements in deep PLL methods when applying the CLSP technique, advocating for a shift from traditional learning-centric approaches to a more data-centric perspective in partial-label learning."}, {"title": "ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs", "pdf_url": "https://openreview.net/attachment?id=xuY33XhEGR&name=pdf", "topics": ["climate modeling", "weather forecasting", "neural ODE", "physics-informed", "uncertainty quantification"], "tldr": "The paper introduces ClimODE, a physics-informed neural model that enhances climate and weather forecasting by incorporating continuous-time dynamics and uncertainty quantification, achieving state-of-the-art performance with significantly fewer parameters than existing data-driven methods.", "summary": "This paper presents ClimODE, a novel approach to climate and weather forecasting that integrates principles from statistical mechanics and utilizes continuous-time neural ODEs to model weather dynamics accurately. ClimODE captures the spatial movement of weather quantities through a value-conserving advection mechanism, allowing for more stable long-term predictions. It employs a hybrid model that combines local convolutions and global attention to account for both regional and global effects, alongside a probabilistic emission model to quantify uncertainty. The authors demonstrate that ClimODE outperforms existing deep learning methods in global and regional forecasting tasks while employing a more efficient parameterization, with open-source implementation made available for further research and application."}, {"title": "Detecting, Explaining, and Mitigating Memorization in Diffusion Models", "pdf_url": "https://openreview.net/attachment?id=84n3UwkH7b&name=pdf", "topics": ["memorization detection", "diffusion models", "text-conditional predictions", "mitigation strategies", "model privacy."], "tldr": "This paper presents an effective approach for detecting and mitigating memorization in diffusion models, utilizing the magnitude of text-conditional predictions to identify and explain memorization while proposing both inference-time and training-time strategies to counteract it.", "summary": "The paper addresses the issue of unintentional memorization in diffusion models, where generated content closely replicates training data, posing legal risks. It introduces a method for detecting memorized prompts by analyzing the magnitude of text-conditional predictions, achieving high detection accuracy even with a single generation per prompt. Additionally, it offers an explainable framework that highlights the contribution of specific tokens to memorization, allowing for user prompt adjustments. Two mitigation strategies are proposed: one for inference that minimizes prediction magnitudes to reduce memorization effects without degrading output quality, and another for training that screens out potentially memorized pairs based on prediction metrics. The results demonstrate the effectiveness of these strategies in maintaining high generative quality while addressing memorization concerns, thus protecting the model's intellectual property and data privacy."}, {"title": "Diffusion Model for Dense Matching", "pdf_url": "https://openreview.net/attachment?id=Zsfiqpft6K&name=pdf", "topics": ["Dense correspondence", "Diffusion model", "Generative prior", "Image matching", "Neural networks"], "tldr": "The paper presents DiffMatch, a novel diffusion-based framework that effectively models both data and prior terms for dense image correspondence, outperforming existing methods in handling matching ambiguities.", "summary": "This paper proposes DiffMatch, a conditional diffusion-based framework aimed at improving dense correspondence between image pairs by explicitly modeling both the data and prior terms, addressing limitations of prior methods that primarily focus on likelihood maximization. Leveraging a conditional denoising diffusion model, the approach incorporates initial correspondence and local matching costs, achieving a more robust representation of the matching field manifold. The authors employ a cascaded architecture to enhance input resolution, starting with a low-resolution model and transitioning to a super-resolution model for flow upsampling. Experimental results demonstrate significant performance improvements on standard benchmarks, particularly in challenging scenarios involving textureless regions and noise, highlighting the effectiveness of the generative prior established by DiffMatch over traditional discriminative methods."}, {"title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation", "pdf_url": "https://openreview.net/attachment?id=UyNXMqnN3c&name=pdf", "topics": ["3D content creation", "Gaussian splatting", "optimization efficiency", "texture refinement", "generative models"], "tldr": "The paper presents DreamGaussian, a framework that efficiently generates high-quality 3D content from images or text by using generative Gaussian splatting combined with novel mesh extraction and texture refinement techniques, achieving significant time savings over existing methods.", "summary": "This paper introduces DreamGaussian, a new framework designed to enhance the efficiency of 3D content creation, particularly for image-to-3D and text-to-3D tasks. Building on recent advancements in generative modeling, the authors propose a two-stage process that first utilizes 3D Gaussian splatting for rapid initialization of geometry and appearance, followed by an effective method for mesh extraction and UV-space texture refinement. Extensive experiments demonstrate that DreamGaussian can produce high-quality, textured 3D meshes from single images or text prompts in just a few minutes, offering around ten times the efficiency compared to previous optimization-based approaches. This work not only addresses challenges related to optimization speed and output quality but also opens new avenues for real-world applications in digital content creation."}, {"title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning", "pdf_url": "https://openreview.net/attachment?id=LjivA1SLZ6&name=pdf", "topics": ["cooperative multi-agent reinforcement learning", "episodic memory", "semantic embeddings", "optimization", "exploration-exploitation"], "tldr": "This paper introduces Efficient episodic Memory Utilization (EMU) for cooperative multi-agent reinforcement learning, enhancing learning speed and policy optimization by utilizing trainable semantic embeddings and an episodic incentive structure.", "summary": "The paper presents Efficient episodic Memory Utilization (EMU) to enhance cooperative multi-agent reinforcement learning (MARL) by addressing the challenges of long convergence times and local optima in complex tasks. EMU incorporates a trainable encoder/decoder structure to create coherent memory embeddings that facilitate better exploration of episodic memory, and introduces an episodic incentive based on the desirability of states to promote desirable transitions. Evaluated in StarCraft II and Google Research Football, EMU demonstrates significant performance improvements over traditional methods, effectively expediting learning and achieving optimal policies. Overall, the contributions of EMU improve memory utilization and reduce the risk of local convergence without requiring extensive hyperparameter tuning."}, {"title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis", "pdf_url": "https://openreview.net/attachment?id=oTRwljRgiv&name=pdf", "topics": ["compositional generalization", "neural program synthesis", "ExeDec", "execution decomposition", "program synthesis tasks"], "tldr": "The paper introduces ExeDec, a novel execution decomposition strategy that improves compositional generalization in neural program synthesis by predicting execution subgoals, significantly enhancing performance compared to baseline methods.", "summary": "This paper addresses the challenge of compositional generalization in neural program synthesis, proposing ExeDec, a strategy that breaks down complex programming tasks into smaller, manageable subtasks informed by execution predictions. The authors establish a meta-benchmark defining five forms of compositional generalization, which they utilize to create tasks for two datasets: RobustFill and DeepCoder. Experimental results demonstrate that ExeDec, when implemented with Transformer models trained from scratch, achieves higher synthesis performance and improved generalization abilities compared to traditional baselines. Furthermore, the paper shows that while large language models (LLMs) struggle with compositional generalization, an adapted ExeDec approach can enhance their performance in few-shot programming tasks. Overall, these findings highlight the importance of task decomposition in advancing neural program synthesis capabilities."}, {"title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!", "pdf_url": "https://openreview.net/attachment?id=hTEGyKf0dZ&name=pdf", "topics": ["fine-tuning", "language models", "safety alignment", "adversarial attacks", "customization"], "tldr": "This paper investigates the safety risks associated with fine-tuning aligned language models, revealing that both malicious and benign fine-tuning can substantially degrade their safety alignment.", "summary": "The paper explores how fine-tuning large language models (LLMs) like GPT-3.5 Turbo and Llama-2 can introduce significant safety risks, even when performed by users with benign intentions. Through red teaming studies, the researchers demonstrate that fine-tuning with a small number of malicious examples can easily compromise the models' safety guardrails, allowing them to produce harmful outputs. Moreover, the study reveals that even fine-tuning with harmless datasets can inadvertently degrade safety alignment due to issues like catastrophic forgetting. The findings underscore a critical gap in existing safety measures that focus only on inference-time defenses, advocating for further research and development of robust safety protocols for the fine-tuning phase. The authors also propose various mitigation strategies to address these emerging risks."}, {"title": "Finetuning Text-to-Image Diffusion Models for Fairness", "pdf_url": "https://openreview.net/attachment?id=hnrB5YHoYu&name=pdf", "topics": ["fairness", "diffusion models", "debiasing", "text-to-image", "distributional alignment"], "tldr": "This work presents a novel approach to mitigate biases in text-to-image diffusion models by framing fairness as a distributional alignment problem, achieving significant reductions in gender and racial biases across varied prompts.", "summary": "The paper addresses the pressing issue of biases in text-to-image (T2I) diffusion models, arguing that these biases can distort societal views and limit opportunities for marginalized groups. The authors propose a dual-method approach: a distributional alignment loss that adjusts the characteristics of generated images towards user-defined target distributions and an adjusted direct finetuning technique that effectively optimizes the image generation process. Empirical results demonstrate that their method significantly reduces gender, racial, and intersectional biases while allowing for flexible control over the representation of various attributes, such as age distribution. The study underscores the importance of social alignment in multimedia generative AI and offers practical tools for achieving fairness in AI-generated content."}, {"title": "Flow Matching on General Geometries", "pdf_url": "https://openreview.net/attachment?id=g7ohDlTITL&name=pdf", "topics": ["Riemannian Flow Matching", "Continuous Normalizing Flows", "Manifolds", "Pre-metrics", "Generative Modeling"], "tldr": "The paper introduces Riemannian Flow Matching (RFM), a novel framework for training continuous normalizing flows on general geometries that is simulation-free, avoids divergence computations, and effectively employs premetrics for training on diverse datasets.", "summary": "This paper presents Riemannian Flow Matching (RFM), a framework designed to improve the training of continuous normalizing flows on Riemannian manifolds. By directly employing a premetric to define target vector fields, RFM circumvents common challenges in generative modeling, such as the need for simulation and costly divergence computations. Notably, RFM operates efficiently on both simple and complex geometries, leveraging exact geodesic computations for straightforward cases and spectral distances for more intricate scenarios. The authors demonstrate RFM's state-of-the-art performance across various real-world non-Euclidean datasets, showcasing its scalability to high dimensions and applicability to complex geometries, including those with boundaries, thus pushing forward the capabilities of deep generative models on manifolds."}, {"title": "Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View", "pdf_url": "https://openreview.net/attachment?id=gFR4QwK53h&name=pdf", "topics": ["gene regulatory networks", "dropout mechanisms", "causal inference", "RNA sequencing", "conditional independence"], "tldr": "This paper presents a causal graphical model to address dropout issues in gene regulatory network inference, demonstrating that conditional independence relations can be reliably estimated by deleting zero-value samples in scRNA-seq data.", "summary": "The paper tackles the challenge of gene regulatory network inference (GRNI) in single-cell RNA sequencing data, which are often affected by dropout errors that produce zeros in gene expression measurements. To address this, the authors introduce a Causal Dropout Model which characterizes the dropout mechanism and establishes that the conditional independence relations derived from the data after excluding samples with zero values are asymptotically equivalent to those from the unaltered dataset. This deletion-based approach can be integrated into existing causal discovery frameworks and has been empirically validated through synthetic, curated, and real-world transcriptomic datasets, showcasing its effectiveness over traditional methods, particularly in preserving accurate causal relationships while mitigating biases introduced by dropouts."}, {"title": "Generalization in diffusion models arises from geometry-adaptive harmonic representations", "pdf_url": "https://openreview.net/attachment?id=ANvmVS2Yr0&name=pdf", "topics": ["diffusion models", "deep neural networks", "generalization", "inductive biases", "image denoising"], "tldr": "The paper demonstrates that deep neural networks (DNNs) trained for image denoising exhibit strong generalization properties and converge to a common score function when trained on large datasets, aided by geometry-adaptive harmonic representations.", "summary": "This paper investigates the generalization capabilities of deep neural networks (DNNs) trained for image denoising, especially in the context of diffusion models. It identifies that when trained on sufficiently large and diverse datasets, DNNs learn nearly identical score functions, leading to the generation of distinct and high-quality samples that are independent of the specific training images. The authors analyze the learned denoising functions to reveal that the DNNs perform a shrinkage operation in bases that adapt to the geometry of the images, termed geometry-adaptive harmonic bases (GAHBs). This suggests that DNNs have strong inductive biases that align well with the true distribution of photographic images, facilitating effective generalization. The study illustrates that while DNN denoisers achieve near-optimal performance for certain classes of images, they may struggle with datasets defined on low-dimensional manifolds where their biases are not fully aligned, leading to suboptimal results."}, {"title": "Generative Modeling with Phase Stochastic Bridge", "pdf_url": "https://openreview.net/attachment?id=tUtGjQEDd4&name=pdf", "topics": ["generative modeling", "phase space dynamics", "stochastic optimal control", "sampling efficiency", "Accelerated Generative Model"], "tldr": "This paper presents the Accelerated Generative Model (AGM), which employs phase space dynamics and stochastic optimal control to enhance sampling efficiency in generative modeling.", "summary": "The paper introduces a new framework for generative modeling called the Accelerated Generative Model (AGM), which is based on the principles of phase space dynamics and stochastic optimal control. It aims to improve sampling efficiency by producing straighter trajectories in momentum systems and by incorporating velocity information to enable early estimation of data points\u2014a technique termed \"sampling-hop.\" The experimental results show that AGM achieves competitive performance in image generation tasks, outperforming existing models, particularly under low function evaluation settings. This indicates its potential as a robust alternative to traditional generative models like diffusion models while highlighting possible avenues for further enhancement in scenarios with more abundant function evaluations."}, {"title": "Ghost on the Shell: An Expressive Representation of General 3D Shapes", "pdf_url": "https://openreview.net/attachment?id=Ad87VjRqUw&name=pdf", "topics": ["3D shapes", "mesh representation", "manifold signed distance field", "generative modeling", "reconstruction"], "tldr": "The paper introduces Ghost-on-the-Shell (G-S HELL), a novel representation for both watertight and non-watertight 3D meshes using manifold signed distance fields, which improves mesh reconstruction and generative capabilities.", "summary": "This paper presents Ghost-on-the-Shell (G-S HELL), a differentiable representation designed to effectively model a wide range of 3D shapes, including both watertight and non-watertight meshes. By defining a manifold signed distance field (mSDF) that characterizes the boundaries of open surfaces on watertight templates, G-S HELL allows for efficient mesh reconstruction from multi-view images and enables generative modeling of 3D shapes. The authors demonstrate that G-S HELL outperforms existing methods in tasks involving reconstruction and unconditional generation of meshes. Their approach facilitates the joint optimization of topology, material, and lighting, advancing the capabilities of 3D shape representation in practical applications."}, {"title": "GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations", "pdf_url": "https://openreview.net/attachment?id=IGzaH538fz&name=pdf", "topics": ["graph neural networks", "adversarial robustness", "certified defense", "graph classification", "perturbation resilience"], "tldr": "GNNCert is a certified defense method for graph neural networks that provides deterministic robustness guarantees against both graph structure and node feature perturbations.", "summary": "This paper introduces GNNCert, a novel certified defense for graph classification that addresses vulnerabilities to adversarial perturbations affecting graph structures and node features. The method involves dividing a testing graph into sub-graphs using a hash function and then employing a base graph classifier to predict labels for these sub-graphs, ultimately using majority voting for the final prediction. Unlike existing approaches, GNNCert offers deterministic robustness guarantees, demonstrating significantly improved certified accuracy across multiple benchmark datasets. The authors validate the effectiveness and efficiency of GNNCert, asserting its potential for broader applications in safety-critical contexts where adversarial resilience is essential."}, {"title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks", "pdf_url": "https://openreview.net/attachment?id=oO6FsMyDBt&name=pdf", "topics": ["graph neural networks", "neural networks", "permutation symmetry", "heterogeneous architectures", "generalization performance"], "tldr": "This paper introduces a neural graph representation for neural networks that preserves permutation symmetry, enabling effective learning and generalization across diverse architectures.", "summary": "The paper addresses the challenge of designing neural networks that process the parameters of other neural networks by proposing a novel neural graph representation, which captures both the architecture and parameters of the original network while preserving permutation symmetry. This representation allows existing graph neural networks (GNNs) and transformers to process heterogeneous architectures, making it feasible to analyze a variety of neural networks with differing structures. The authors demonstrate the effectiveness of their approach on multiple tasks, including classification of implicit neural representations and predicting generalization performance, achieving substantial improvements over state-of-the-art methods. The results highlight the potential of their model for various applications in the field of geometric deep learning."}, {"title": "How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models", "pdf_url": "https://openreview.net/attachment?id=pzElnMrgSD&name=pdf", "topics": ["noise prior", "diffusion models", "temporal coherence", "video generation", "noise transport"], "tldr": "This paper presents a novel noise representation called R-noise that preserves temporal correlations in diffusion models for video editing and generation, improving quality by reducing flickering and texture-sticking artifacts.", "summary": "The paper addresses the limitations of existing diffusion-based video processing methods, which struggle to maintain temporal coherence due to ineffective noise sampling techniques. It introduces a new noise representation, R-noise, which interprets noise samples as a continuously integrated noise field, thereby allowing for accurate transport of noise samples across frames while preserving their correlations and properties. The authors derive a noise transport equation and implement a practical algorithm to apply this representation in various video tasks, such as video restoration and conditional generation. Experimental results demonstrate substantial improvements over traditional noise priors, showcasing the effectiveness of R-noise in achieving temporally coherent and visually high-quality video outputs."}, {"title": "How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?", "pdf_url": "https://openreview.net/attachment?id=AhizIPytk4&name=pdf", "topics": ["3D medical imaging", "transfer learning", "supervised pre-training", "AbdomenAtlas 1.1", "segmentation performance"], "tldr": "The paper presents AbdomenAtlas 1.1, a large, annotated dataset for 3D medical imaging, and demonstrates that models pre-trained on this dataset significantly outperform existing pre-trained models in transfer learning for image segmentation tasks.", "summary": "This study investigates the effectiveness of supervised pre-training for 3D image segmentation by creating AbdomenAtlas 1.1, a dataset comprising 9,262 CT volumes with detailed voxel-level annotations for 25 anatomical structures and pseudo annotations for tumor types. The authors develop a suite of supervised pre-trained models (SuPreM) that leverage this extensive dataset and showcase significant efficiency benefits in transfer learning compared to self-supervised methods; for example, models trained with only 21 CT volumes can achieve similar performance as those trained on much larger datasets. Their findings indicate that the supervised pre-training approach enhances model transferability across different medical imaging tasks, and the released pre-trained models are expected to boost advancements in 3D segmentation capabilities in the medical domain."}, {"title": "Improved Active Learning via Dependent Leverage Score Sampling", "pdf_url": "https://openreview.net/attachment?id=IYxDy2jDFL&name=pdf", "topics": ["active learning", "leverage score sampling", "pivotal sampling", "polynomial regression", "agnostic learning"], "tldr": "The paper presents a pivotal sampling method that integrates spatial awareness with leverage score sampling, achieving improved sample efficiency in active learning problems, particularly in polynomial regression settings.", "summary": "This research addresses the active linear regression problem within the agnostic learning setting, proposing a novel pivotal sampling technique that enhances traditional leverage score sampling by ensuring spatial coverage of samples. The authors demonstrate that this method can reduce the number of samples required to achieve a specified accuracy by up to 50% compared to independent sampling methods. The approach is theoretically supported by two main results: one that matches the sampling complexity of independent leverage score sampling and another that provides improved bounds for polynomial regression. Through empirical tests on various problems, including those motivated by parametric PDEs, the pivotal sampling method outperformed standard Bernoulli leverage score sampling, showcasing its practical effectiveness in approximating complex functions. This work suggests a promising direction for combining theoretical rigor with practical sampling strategies in active learning contexts."}, {"title": "Improved Techniques for Training Consistency Models", "pdf_url": "https://openreview.net/attachment?id=WNzy9bRDvG&name=pdf", "topics": ["consistency models", "training techniques", "generative models", "Pseudo-Huber loss", "sample quality"], "tldr": "The paper presents significant improvements in training consistency models for generative tasks, surpassing prior methods and achieving state-of-the-art performance without relying on adversarial training or learned metrics.", "summary": "This paper focuses on enhancing consistency training (CT) techniques for generative models, aiming to match or exceed the performance of consistency distillation (CD) while avoiding its computational overhead and quality limitations. The authors identify and address a flaw in previous theoretical analyses by eliminating Exponential Moving Average from the teacher model and introducing robust Pseudo-Huber loss functions. They propose an innovative curriculum for total discretization steps and a lognormal noise schedule, resulting in improved sample quality. Through extensive experiments, the models achieve Fr\u00e9chet Inception Distance (FID) scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64x64, respectively, significantly outperforming prior methods. These results illustrate the potential of consistency models as a promising independent family of generative models, rivaling other state-of-the-art approaches."}, {"title": "Improving Convergence and Generalization Using Parameter Symmetries", "pdf_url": "https://openreview.net/attachment?id=L0r0GphlIL&name=pdf", "topics": ["teleportation", "optimization", "parameter symmetries", "generalization", "neural networks"], "tldr": "This paper demonstrates that incorporating parameter space symmetries through a method called \"teleportation\" enhances the convergence speed of optimization algorithms and improves the generalization of neural networks.", "summary": "The research investigates the role of parameter space symmetries in deep neural networks, specifically through a technique known as teleportation, which accelerates optimization by moving parameters to steeper points in loss landscapes. The authors provide theoretical guarantees that teleportation boosts convergence rates in stochastic gradient descent (SGD) and can lead to optimal trajectories for certain loss functions. Additionally, the paper explores the relationship between the curvature of minima and generalization, revealing that teleporting to regions with different curvature can enhance a model's ability to generalize. Various optimization algorithms are shown to benefit from integrating teleportation, highlighting its versatility and potential applications in improving convergence and generalization in neural network training."}, {"title": "InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning", "pdf_url": "https://openreview.net/attachment?id=C61sk5LsK6&name=pdf", "topics": ["data pruning", "lossless performance", "training acceleration", "gradient expectation", "unbiased framework"], "tldr": "The paper introduces InfoBatch, a framework that leverages unbiased dynamic data pruning to achieve lossless training acceleration across various deep learning tasks.", "summary": "This paper presents InfoBatch, a novel data pruning method designed to accelerate training without sacrificing performance. It addresses the issue of gradient expectation bias associated with traditional static and dynamic pruning approaches by employing a soft pruning strategy that randomly discards less informative samples while rescaling the gradients of the remaining data. This approach allows InfoBatch to maintain a similar gradient expectation to that of training on the full dataset. The framework is versatile and achieves lossless performance with 20% to 40% less overall cost across multiple tasks, including classification and semantic segmentation, as demonstrated through experiments on datasets like CIFAR and ImageNet. The authors conclude that InfoBatch significantly enhances training efficiency while minimizing computational overhead, making it suitable for real-world applications in deep learning."}, {"title": "Interpreting CLIP's Image Representation via Text-Based Decomposition", "pdf_url": "https://openreview.net/attachment?id=5Ca9sSzuDp&name=pdf", "topics": ["CLIP", "image representation", "attention heads", "text-based decomposition", "zero-shot segmentation"], "tldr": "This paper analyzes CLIP's image representation by decomposing it into contributions from individual layers, attention heads, and image patches, revealing specialized roles and enabling performance improvements in zero-shot image segmentation and spurious feature reduction.", "summary": "The paper explores the internal mechanisms of CLIP's image encoder by dissecting its representation into components influenced by specific layers, attention heads, and image locations. Utilizing a method called TEXTSPAN, the authors label and interpret attention heads based on their output characteristics, identifying property-specific functions like color and shape recognition. The study reveals that focusing on certain layers and heads significantly affects classification accuracy, leading to enhanced performance in zero-shot segmentation tasks and mitigation of spurious correlations in outputs. The findings suggest that a deeper understanding of transformer models like CLIP can facilitate improvements in their effectiveness for various downstream applications."}, {"title": "Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video", "pdf_url": "https://openreview.net/attachment?id=Yen1lGns2o&name=pdf", "topics": ["self-supervised learning", "image encoding", "video data", "DORA", "Walking Tours dataset"], "tldr": "This paper introduces the Walking Tours dataset and a novel self-supervised learning method called DORA, demonstrating that training on a single lengthy, unlabeled video can yield performance competitive with traditional ImageNet pretraining for various image and video tasks.", "summary": "The paper investigates the efficiency of data usage in self-supervised learning by introducing a new dataset, Walking Tours, comprising high-resolution, continuously captured first-person videos. It presents a self-supervised image pretraining method named DORA, which focuses on tracking and recognizing objects across video frames using transformer cross-attention mechanisms. The results show that the DORA method enables competitive performance with ImageNet-pretrained models by using just a single Walking Tours video for training. The findings suggest a shift towards more effective use of video data in representation learning, potentially accelerating the training process compared to conventional methods reliant on large-scale image datasets."}, {"title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models", "pdf_url": "https://openreview.net/attachment?id=WbWtOYIzIK&name=pdf", "topics": ["knowledge cards", "large language models", "modular frameworks", "knowledge integration", "factuality"], "tldr": "The paper introduces KNOWLEDGE CARD, a framework that enhances large language models (LLMs) by integrating specialized, modular knowledge cards, enabling dynamic knowledge synthesis and updates.", "summary": "This paper presents the KNOWLEDGE CARD framework, which addresses the limitations of static large language models (LLMs) in knowledge-intensive tasks by incorporating modular knowledge cards\u2014specialized language models trained on specific domains. The framework enables flexible knowledge integration through two main approaches: bottom-up and top-down\u2014allowing LLMs to dynamically access relevant information while controlling for relevance, brevity, and factuality via three implemented selectors. Extensive experiments demonstrate KNOWLEDGE CARD's superior performance over standard LLMs and existing retrieval-augmented models across multiple tasks, significantly improving responses, especially in areas requiring current knowledge updates. The approach fosters community-driven contributions to LLM knowledge enhancement and aims to lower the carbon footprint associated with retraining large models."}, {"title": "Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time", "pdf_url": "https://openreview.net/attachment?id=bTMMNT7IdW&name=pdf", "topics": ["Evolving Domain Generalization", "Stochastic Differential Equations", "Infinitely Fined-Grid Evolving Trajectory", "Distribution Shift", "Machine Learning."], "tldr": "The paper proposes SDE-EDG, a novel approach for Evolving Domain Generalization that utilizes Stochastic Differential Equations and continuous interpolations to better model evolving patterns in data under distribution shifts over time.", "summary": "This study addresses the challenges of Evolving Domain Generalization (EDG) in machine learning, particularly in scenarios with limited timestamps leading to overfitting and poor generalization. The authors introduce a new method, SDE-EDG, which constructs an Infinitely Fined-Grid Evolving Trajectory (IFGET) using sample-to-sample correspondence and continuous interpolation techniques to bridge temporal gaps. By leveraging Stochastic Differential Equations, SDE-EDG models the evolving dynamics of latent representations, enhancing the model's ability to capture distribution shifts over time. Evaluations on benchmark datasets demonstrate that SDE-EDG significantly outperforms existing state-of-the-art methods, highlighting its effectiveness in generalizing to unseen domains. The approach is further supported by theoretical analysis showing its potential to mitigate generalization risks related to time-varying environments."}, {"title": "Learning Energy Decompositions for Partial Inference in GFlowNets", "pdf_url": "https://openreview.net/attachment?id=P15CHILQlg&name=pdf", "topics": ["GFlowNets", "energy decomposition", "partial inference", "potential functions", "training algorithms"], "tldr": "This paper introduces Learning Energy Decompositions for GFlowNets (LED-GFN), enhancing the sampling efficiency from the Boltzmann distribution by employing learnable potential functions for improving credit assignment during training.", "summary": "This paper explores the use of Generative Flow Networks (GFlowNets) for sampling objects from a Boltzmann energy distribution through sequences of actions, with a focus on enhancing training via partial inference. The authors propose LED-GFN, which overcomes the limitations of previous methods by decomposing terminal state energy into learnable potential functions associated with state transitions, thus facilitating better local credit assignments. Empirical evaluations across various tasks, including bag generation, molecular discovery, and RNA sequence generation, demonstrate that LED-GFN consistently outperforms existing approaches, even when traditional methods have access to ideal local credits, highlighting its effectiveness and practicality in diverse sampling applications."}, {"title": "Learning Interactive Real-World Simulators", "pdf_url": "https://openreview.net/attachment?id=sFyTZEqmUY&name=pdf", "topics": ["universal simulator", "generative modeling", "real-world interactions", "reinforcement learning", "vision-language policy"], "tldr": "The paper presents a universal simulator (UniSim) that integrates diverse datasets to simulate realistic experiences of real-world interactions, enabling the training of both vision-language and low-level control policies for deployment in actual scenarios.", "summary": "This paper explores the development of a universal simulator (UniSim) capable of generating realistic visual outcomes in response to various human and robotic actions, leveraging generative modeling techniques. By orchestrating a wide range of datasets, which include textual, visual, and behavioral information, the simulator is designed to facilitate interactive learning and task execution. The authors demonstrate UniSim's effectiveness in training high-level vision-language policies and low-level reinforcement learning agents, showing that these agents can generalize to real-world applications after training purely in simulation. The findings suggest broader implications for the use of such simulators in improving machine intelligence, making them applicable in fields ranging from robotics to content creation."}, {"title": "LEGO-Prover: Neural Theorem Proving with Growing Libraries", "pdf_url": "https://openreview.net/attachment?id=3f5PALef5B&name=pdf", "topics": ["neural theorem proving", "large language models", "skill library", "modular proof construction", "formalization"], "tldr": "The paper presents LEGO-Prover, a novel approach to neural theorem proving that utilizes a growing skill library to enhance the capability of large language models in formalizing mathematical proofs.", "summary": "This paper introduces LEGO-Prover, an innovative method for neural theorem proving that addresses the challenges faced by existing models due to their reliance on static libraries. By employing a growing library of verified lemmas as modular skills, LEGO-Prover allows for the construction of proofs in a block-by-block manner, enabling the model to retrieve existing skills and create new ones during the proving process. The system demonstrated significant advancements in success rates on the miniF2F dataset, achieving a pass rate of 57.0% on the validation set and 50.0% on the test set, surpassing previous state-of-the-art approaches. The findings suggest that the dynamically expanding skill library enhances the ability of language models to tackle increasingly complex mathematical problems and bridge the gap between informal and formal proofs."}, {"title": "Less is More: Fewer Interpretable Region via Submodular Subset Selection", "pdf_url": "https://openreview.net/attachment?id=jKTUlxo5zy&name=pdf", "topics": ["image attribution", "submodular optimization", "interpretability", "machine learning", "explanation methods"], "tldr": "This paper introduces a novel method for image attribution based on submodular subset selection, improving interpretability with fewer, more accurate regions and addressing both correct and incorrect model predictions.", "summary": "This paper addresses challenges in existing image attribution methods, specifically the tendency to generate imprecise attribution regions and difficulties in interpreting incorrect model predictions. It reformulates the image attribution problem as a submodular subset selection task, employing a new submodular function that considers prediction confidence, effectiveness, consistency, and collaboration of sub-regions. Extensive experiments on two face datasets (Celeb-A and VGG-Face2) and a fine-grained dataset (CUB-200-2011) demonstrate significant performance improvements in Deletion and Insertion AUC scores, with the proposed method outperforming state-of-the-art approaches, and effectively identifying causes behind misclassifications. This work enhances the interpretability of machine learning models while illustrating the importance of local region analysis in attribution tasks."}, {"title": "Lipschitz Singularities in Diffusion Models", "pdf_url": "https://openreview.net/attachment?id=WNkW0cOwiz&name=pdf", "topics": ["Diffusion models", "Lipschitz singularities", "E-TSDM", "stability", "image synthesis"], "tldr": "The paper addresses the issue of infinite Lipschitz constants near the zero point in diffusion models and proposes a novel method, E-TSDM, to mitigate these singularities, leading to significant performance improvements in image synthesis tasks.", "summary": "This paper investigates the problem of infinite Lipschitz constants in diffusion models, particularly near the zero time point, which negatively impacts the models' stability and accuracy during training and inference. The authors provide both theoretical proofs and empirical evidence for the presence of Lipschitz singularities and propose the Early Timestep-shared Diffusion Model (E-TSDM), which alleviates these issues by sharing timestep conditions in intervals with high Lipschitz constants. Extensive experiments across various datasets demonstrate that E-TSDM significantly enhances performance, reduces Fr\u00e9chet Inception Distance in image generation tasks by over 33%, and maintains adaptability across different noise schedules and fast sampling methods. The findings advance understanding of diffusion processes and highlight potential pathways for further model improvements."}, {"title": "LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models", "pdf_url": "https://openreview.net/attachment?id=aIok3ZD9to&name=pdf", "topics": ["carbon footprint", "large language models", "LLMCarbon", "operational emissions", "embodied emissions"], "tldr": "This paper introduces LLMCarbon, a comprehensive model for accurately predicting the carbon footprint of large language models (LLMs) during their training, inference, experimentation, and storage phases, addressing gaps in existing tools.", "summary": "The paper presents LLMCarbon, an end-to-end carbon footprint projection model specifically designed for large language models (LLMs), both dense and mixture-of-experts (MoE) architectures. The model incorporates essential LLM, hardware, and data center parameters to estimate both operational and embodied carbon emissions throughout the LLM's lifecycle. Compared to existing tools like mlco2, LLMCarbon demonstrates improved accuracy in carbon footprint estimations, with validation results showing discrepancies of only 8.2% from actual data. The findings emphasize the significance of considering both operational efficiency and embodied carbon in the assessment of LLMs' environmental impacts, thereby supporting more sustainable AI practices moving forward."}, {"title": "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models", "pdf_url": "https://openreview.net/attachment?id=LzPWWPAdY4&name=pdf", "topics": ["quantization", "LoRA", "language models", "fine-tuning", "performance improvement"], "tldr": "This paper presents LoftQ, a novel quantization framework that improves the performance of quantized large language models through LoRA fine-tuning by simultaneously mitigating the discrepancies introduced by quantization.", "summary": "The authors introduce LoftQ, a novel quantization framework designed to effectively combine quantization and LoRA fine-tuning in large language models (LLMs). Recognizing the performance gaps often observed when using low-bit quantization methods in conjunction with LoRA, LoftQ aims to provide a more suitable low-rank initialization that reduces discrepancies between quantized and full-precision models. The technique involves alternating between quantization and low-rank approximation to better approximate high-precision weights. Extensive experiments demonstrate that LoftQ significantly outperforms existing quantization approaches like QLoRA across various natural language understanding, question answering, summarization, and generation tasks, showcasing robust performance particularly in low-bit quantization scenarios such as 2-bit precision."}, {"title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models", "pdf_url": "https://openreview.net/attachment?id=6PmJoRfdaK&name=pdf", "topics": ["LongLoRA", "context extension", "sparse attention", "fine-tuning", "large language models"], "tldr": "LongLoRA is an efficient fine-tuning method that enables large language models to extend their context lengths significantly with reduced computational costs and minimal accuracy loss.", "summary": "This paper presents LongLoRA, a novel approach designed to efficiently extend the context lengths of pre-trained large language models (LLMs) while minimizing computational expenses. The authors introduce Shifted Sparse Attention (S2-Attn) to enhance context extension using sparse local attention during fine-tuning without altering the global dense attention required during inference. They also improve the parameter-efficient fine-tuning technique Low-Rank Adaptation (LoRA) by making embedding and normalization layers trainable, which effectively bridges the performance gap between conventional LoRA and full fine-tuning. Empirical results demonstrate that LongLoRA successfully fine-tunes LLaMA models, extending the context from shorter lengths to up to 100k tokens for LLaMA 7B models on a single 8 A100 machine, while retaining significant performance. The work also highlights compatibility with existing optimization techniques and outlines future research directions."}, {"title": "LRM: Large Reconstruction Model for Single Image to 3D", "pdf_url": "https://openreview.net/attachment?id=sllU8vvsFF&name=pdf", "topics": ["3D Reconstruction", "Neural Radiance Fields", "Transformer Architecture", "Image-to-3D", "Large-Scale Learning"], "tldr": "The paper introduces the Large Reconstruction Model (LRM), a transformer-based framework that efficiently generates high-quality 3D shapes from single images within five seconds, leveraging a large-scale dataset of nearly one million images and 3D objects.", "summary": "This paper presents LRM, the first large-scale system for reconstructing 3D models from single images using a scalable transformer-based architecture with 500 million parameters. Unlike preceding methods that required category-specific training on smaller datasets, LRM is trained end-to-end on a diverse dataset combining synthetic and real images to create a generalizable 3D prior. It employs a visual transformer to encode image features and a decoder that maps these features to a triplane representation, which is then used to render 3D shapes. The model demonstrates high fidelity in reconstructing complex geometries and textures from various input types, including real-world images and those generated by other models, thus offering significant potential for applications in design and augmented reality."}, {"title": "Mastering Memory Tasks with World Models", "pdf_url": "https://openreview.net/attachment?id=1vDArHJ68h&name=pdf", "topics": ["model-based reinforcement learning", "long-term memory", "state space models", "credit assignment", "world models"], "tldr": "The paper introduces Recall to Imagine (R2I), a novel approach that integrates a modified state space model into model-based reinforcement learning to enhance long-term memory and improve credit assignment in various complex tasks.", "summary": "This paper presents R2I, a new method that enhances model-based reinforcement learning (MBRL) agents' ability to handle long-term dependencies by integrating a variant of state space models (SSMs) within the world models of these agents. R2I demonstrates superior performance in memory-intensive tasks like BSuite and POPGym, achieving state-of-the-art results, including surpassing human performance in the Memory Maze benchmark. The method maintains competitive performance in conventional RL environments like Atari and DMC while significantly improving computational speed, allowing faster convergence than previous state-of-the-art methods. By systematically demonstrating R2I's capabilities across various domains, the authors emphasize its potential for improved long-term memory and credit assignment in reinforcement learning applications."}, {"title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts", "pdf_url": "https://openreview.net/attachment?id=KUNzEQMWU7&name=pdf", "topics": ["mathematical reasoning", "foundation models", "visual contexts", "benchmark evaluation", "MATHVISTA"], "tldr": "This paper introduces MATHVISTA, a benchmark for evaluating the mathematical reasoning capabilities of foundation models in visual contexts, revealing significant performance gaps compared to humans and highlighting the strengths of the GPT-4V model.", "summary": "The paper presents MATHVISTA, a benchmark created to systematically assess the mathematical reasoning abilities of large language models (LLMs) and large multimodal models (LMMs) in visual contexts. The dataset comprises 6,141 examples derived from 28 existing multimodal datasets and three newly created datasets, covering various mathematical reasoning types and visual contexts. A comprehensive evaluation of 12 foundation models, including GPT-4V and Bard, reveals that while GPT-4V achieved the highest performance at 49.9%, it still falls short of human performance by 10.4%. The findings underscore the ongoing challenges in enabling AI models to effectively integrate visual comprehension with rigorous mathematical reasoning and highlight the potential for further advancements in this domain."}, {"title": "Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction", "pdf_url": "https://openreview.net/attachment?id=TpD2aG1h0D&name=pdf", "topics": ["continual learning", "Meta-Continual Learning", "Hessian approximation", "variance reduction", "online learning"], "tldr": "This paper introduces Variance-Reduced Meta-CL (VR-MCL), a new method that combines the principles of Meta-Continual Learning and regularization-based approaches to enhance Hessian approximation while reducing variance, significantly improving performance in continual learning tasks.", "summary": "The paper revisits the field of continual learning (CL) by bridging regularization-based methods with Meta-Continual Learning (Meta-CL). It highlights the limitations of existing Hessian approximations used in regularization methods, which remain fixed during training, and discusses how Meta-CL leverages hypergradient information for more timely Hessian updates but suffers from high variance due to sampling inconsistencies. To address these challenges, the authors propose Variance-Reduced Meta-CL (VR-MCL), which effectively reduces hypergradient variance through a momentum-based technique, concurrently providing a more accurate approximation of the Hessian. Comprehensive experiments demonstrate that VR-MCL outperforms state-of-the-art CL methods across various datasets and settings, while theoretical analysis reinforces its efficacy and provides a regret bound, confirming VR-MCL's superiority in maintaining knowledge over sequential tasks."}, {"title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework", "pdf_url": "https://openreview.net/attachment?id=VtmBAGCN7o&name=pdf", "topics": ["meta-programming", "multi-agent systems", "large language models", "software engineering", "standardized operating procedures"], "tldr": "The paper introduces MetaGPT, a meta-programming framework that improves multi-agent collaboration and code generation quality through structured workflows and executable feedback mechanisms.", "summary": "This paper presents MetaGPT, a meta-programming framework designed to enhance the capabilities of multi-agent systems utilizing large language models (LLMs) by integrating Standardized Operating Procedures (SOPs) into workflows. The framework assigns specialized roles to agents, allowing them to collaboratively tackle complex software engineering tasks more efficiently. MetaGPT employs structured communication protocols and incorporates an executable feedback mechanism that iteratively improves code quality. Experimental evaluations demonstrate that MetaGPT achieves state-of-the-art performance on various coding benchmarks, highlighting its robustness and suitability for automating complex problem-solving processes in software development."}, {"title": "METRA: Scalable Unsupervised RL with Metric-Aware Abstraction", "pdf_url": "https://openreview.net/attachment?id=c5pwL0Soay&name=pdf", "topics": ["Unsupervised Reinforcement Learning", "Metric-Aware Abstraction", "Skill Discovery", "High-Dimensional Environments", "Temporal Distances"], "tldr": "The paper introduces METRA, a scalable unsupervised reinforcement learning method that achieves diverse skill discovery in complex, high-dimensional environments by utilizing a metric-aware objective based on temporal distances.", "summary": "This paper presents METRA, an innovative unsupervised reinforcement learning (RL) framework designed to effectively explore and learn diverse behaviors in complex, high-dimensional environments without supervision. Traditional unsupervised RL approaches, such as pure exploration and mutual information skill discovery, struggle to scale in such settings due to the challenges of covering large state spaces or incentivizing effective exploration. METRA overcomes these limitations by focusing on a compact latent space connected to the state space through temporal distances, maximizing coverage and discovering a variety of useful skills. Experiments across five locomotion and manipulation environments demonstrate that METRA successfully identifies diverse behaviors, outperforming existing unsupervised RL methods and providing a foundation for efficient learning in downstream tasks."}, {"title": "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space", "pdf_url": "https://openreview.net/attachment?id=4Ay23yeuz0&name=pdf", "topics": ["tabular data synthesis", "diffusion models", "variational autoencoder", "synthetic data quality", "mixed data types"], "tldr": "This paper introduces TABSYN, an advanced methodology for synthesizing tabular data using a diffusion model within a variational autoencoder framework, achieving significant improvements in data quality and generation speed over existing methods.", "summary": "The paper presents TABSYN, a novel approach for generating synthetic tabular data that effectively handles mixed data types by first transforming raw tabular data into a continuous embedding space and then applying a score-based diffusion model in this latent representation. Key innovations of TABSYN include the integration of a variational autoencoder with a transformer architecture to capture inter-column relationships and adaptive loss weighting to optimize the latent space distribution. Experiments across six datasets show that TABSYN outperforms state-of-the-art methods, reducing error rates by 86% for column-wise density estimation and 67% for pair-wise correlations, while also demonstrating efficient performance on downstream tasks like machine learning and missing value imputation. The results highlight the potential of TABSYN to generate high-quality synthetic data that captures the nuances of real-world tabular datasets."}, {"title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs", "pdf_url": "https://openreview.net/attachment?id=uNrFpDPMyo&name=pdf", "topics": ["adaptive KV cache compression", "large language models", "generative inference", "memory efficiency", "attention profiling"], "tldr": "This paper introduces FastGen, an adaptive method for compressing the KV cache in large language models that significantly reduces memory usage without sacrificing generation quality.", "summary": "The paper presents FastGen, a novel approach for adaptive KV cache compression aimed at improving the memory efficiency during generative inference of large language models (LLMs). By leveraging lightweight attention profiling to identify the unique structures of various attention heads, FastGen selectively evicts less relevant tokens from the cache based on the recognized patterns. The proposed method demonstrates a substantial reduction in GPU memory consumption, achieving over 95% recovery of attention scores while compressing the cache by up to 50%, all without the need for resource-intensive fine-tuning or retraining. Experimental results across multiple tasks show that FastGen outperforms traditional fixed KV cache methods, highlighting its potential for practical implementations in LLMs."}, {"title": "Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems.", "pdf_url": "https://openreview.net/attachment?id=nHESwXvxWK&name=pdf", "topics": ["Bayesian inference", "linear inverse problems", "denoising diffusion models", "Sequential Monte Carlo", "uncertainty quantification"], "tldr": "This paper introduces MCGdiff, a novel Sequential Monte Carlo algorithm that samples from the Bayesian posterior of linear inverse problems with denoising diffusion model priors, demonstrating superior performance against competing methods.", "summary": "This paper addresses ill-posed linear inverse problems commonly found in applications like medical imaging and computational photography by leveraging Bayesian inference with denoising diffusion models as priors. The authors propose MCGdiff, a new algorithm that sequentially estimates the posterior distribution of the unknowns by exploiting the specific structure of the prior and employing Sequential Monte Carlo methods. Theoretical foundations support the convergence of MCGdiff to the target posterior, and numerical experiments showcase its effectiveness over existing methods, confirming that MCGdiff consistently yields high-quality samples aligned with the target distribution. Overall, this work contributes to advancing approaches for robust solutions in Bayesian linear inverse problems, emphasizing the importance of accurate uncertainty quantification."}, {"title": "Multi-granularity Correspondence Learning from Long-term Noisy Videos", "pdf_url": "https://openreview.net/attachment?id=9Cu8MRmhq2&name=pdf", "topics": ["video-language", "temporal learning", "optimal transport", "noisy correspondence", "action segmentation"], "tldr": "The paper introduces Norton, a method that utilizes noise-robust temporal optimal transport to effectively learn temporal correlations from long videos while addressing multi-granularity noisy correspondences.", "summary": "This paper addresses the challenge of learning long-term temporal dependencies in video-language understanding, which is often overlooked due to computational limitations. The proposed method, Norton, leverages a unified optimal transport framework to tackle the multi-granularity noisy correspondence (MNC) problem, which encompasses both coarse-grained clip-caption misalignments and fine-grained frame-word misalignments. Norton employs contrastive losses, an alignable prompt bucket, and a soft-maximum operator to filter out irrelevant pairs and identify crucial words and frames. Comprehensive experiments on various video tasks, including video-paragraph retrieval and action segmentation, demonstrate that Norton outperforms existing approaches, indicating its effectiveness in capturing long-term temporal relations while mitigating alignment noise."}, {"title": "Multi-Source Diffusion Models for Simultaneous Music Generation and Separation", "pdf_url": "https://openreview.net/attachment?id=h922Qhkmx1&name=pdf", "topics": ["music generation", "source separation", "diffusion models", "generative models", "audio processing"], "tldr": "The paper presents a novel multi-source diffusion model that simultaneously performs music generation, source separation, and source imputation by learning the joint probability density of contextual audio sources.", "summary": "This work introduces a diffusion-based generative model capable of handling multiple audio tasks, specifically music synthesis and source separation. By learning the joint distribution of instrumental sources within a musical context, the model enables various inference tasks such as total generation (creating a complete mixture), partial generation (generating missing sources given others), and source separation (isolating sources from a mixture). The proposed model, trained on the Slakh2100 dataset, utilizes an innovative Dirac likelihood-based method for source separation, achieving competitive results against state-of-the-art methods while allowing for flexible audio manipulation. This advancement highlights the potential for developing general audio models that facilitate complex music composition and source control."}, {"title": "Multisize Dataset Condensation", "pdf_url": "https://openreview.net/attachment?id=FVhmnvqnsI&name=pdf", "topics": ["dataset condensation", "on-device learning", "subset degradation", "adaptive subset loss", "computational efficiency"], "tldr": "The paper presents a novel approach called Multisize Dataset Condensation (MDC) that condenses multiple dataset adjustment processes into a single one to facilitate flexible dataset sizes for on-device learning while overcoming the subset degradation problem.", "summary": "The paper addresses the challenges of dataset condensation in on-device scenarios, where fluctuating computational resources require flexible dataset sizes and limit additional condensation operations. The authors propose the Multisize Dataset Condensation (MDC) method, which consolidates N condensation processes into one by introducing an adaptive subset loss to mitigate subset degradation\u2014where selecting a subset from a condensed dataset often leads to lower performance than condensing directly from the full dataset. The results demonstrate that MDC outperforms traditional methods in various network architectures and datasets, achieving significant accuracy gains without the overhead of multiple condensation processes. Overall, MDC presents a more efficient and effective solution for dataset condensation in resource-constrained environments."}, {"title": "Neural Fine-Tuning Search for Few-Shot Learning", "pdf_url": "https://openreview.net/attachment?id=T7YV5UZKBc&name=pdf", "topics": ["Few-Shot Learning", "Neural Architecture Search", "Adaptation Strategies", "Meta-Dataset", "Fine-Tuning"], "tldr": "This paper introduces Neural Fine-Tuning Search (NFTS), a method that utilizes neural architecture search to optimally determine which layers of pre-trained models to adapt in few-shot learning, achieving state-of-the-art performance on established benchmarks.", "summary": "The paper addresses the challenge of few-shot learning, where classifiers must rapidly adapt to new, disjoint class sets. It proposes a novel approach called Neural Fine-Tuning Search (NFTS) that employs neural architecture search (NAS) to systematically identify the most effective adaptation strategies\u2014deciding which model layers to fine-tune and where to insert new parameters (adapters). By leveraging both residual networks and vision transformers, the authors show that NFTS outperforms existing methods on the Meta-Dataset and Meta-Album benchmarks. The study provides empirical evidence that diversely selected architectures enhance performance and offers insights into the optimal configuration for few-shot adaptation across different domains."}, {"title": "Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors", "pdf_url": "https://openreview.net/attachment?id=PdaPky8MUn&name=pdf", "topics": ["long-range dependencies", "self-pretraining", "Transformers", "state space models", "performance evaluation"], "tldr": "This paper demonstrates that self-pretraining significantly improves the performance of Transformers and state space models on long-sequence tasks, challenging the practice of training architectures from scratch.", "summary": "The paper addresses the overestimation of performance differences among various sequence models, notably Transformers and state space models (SSMs), when trained from random initialization. It highlights the effectiveness of self-pretraining (SPT) using denoising objectives, which allows models to leverage only downstream task data for initialization. Through comprehensive evaluations on long sequence benchmarks, particularly the Long Range Arena (LRA), the authors show that proper pretraining leads to substantial performance gains for both Transformers and SSMs, narrowing the performance gaps and even achieving state-of-the-art results for certain tasks. The study emphasizes that the inclusion of data-driven priors through pretraining is essential for reliable performance assessments and suggests that manual architectural modifications may become redundant in light of effective self-pretraining."}, {"title": "On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs", "pdf_url": "https://openreview.net/attachment?id=H3UayAQWoE&name=pdf", "topics": ["Large Language Models", "Psychological Evaluation", "PsychoBench", "Psychometrics", "Emotional Intelligence"], "tldr": "This paper introduces PsychoBench, a framework for evaluating the psychological portrayal of Large Language Models (LLMs) across various psychological domains using established psychometric scales.", "summary": "The paper proposes PsychoBench, a comprehensive framework designed to assess the psychological aspects of Large Language Models (LLMs) through thirteen widely used psychometric scales categorized into personality traits, interpersonal relationships, motivational tests, and emotional abilities. The authors evaluate five prominent LLMs, including text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b, and employ a jailbreaking technique to reveal the models' intrinsic characteristics. Results indicate significant differences in psychological profiles based on model size, updates, and the application of jailbreak methods, showing LLMs generally display higher openness, conscientiousness, and emotional intelligence than average humans. This study concludes with implications for developing more empathetic and engaging AI systems and emphasizes the importance of understanding the psychological attributes of LLMs to ensure ethical and responsible deployment in society."}, {"title": "On the Joint Interaction of Models, Data, and Features", "pdf_url": "https://openreview.net/attachment?id=ze7DOLi394&name=pdf", "topics": ["feature learning", "interaction tensor", "deep learning", "Generalization Disagreement Equality", "model data interactions"], "tldr": "This paper introduces the interaction tensor as a novel tool for analyzing feature learning in deep learning models, revealing important insights about features, their distribution in data, and the implications for generalization and model performance.", "summary": "The paper addresses the theoretical understanding of feature learning in deep learning, a field that lacks comprehensive formalizations despite its empirical successes. It introduces the interaction tensor to empirically analyze how features are learned from data across different models. Through this framework, the authors observe that features follow a long-tailed distribution, with models exhibiting different behaviors based on the features they learn, and they demonstrate the connection to the Generalization Disagreement Equality (GDE) without assuming model calibration. The proposed framework successfully captures various phenomena in feature learning, offering insights into how model architectures, data distributions, and feature characteristics interact, while providing a tool for further research into feature learning and model behavior."}, {"title": "One-shot Empirical Privacy Estimation for Federated Learning", "pdf_url": "https://openreview.net/attachment?id=0BqyZSWfzo&name=pdf", "topics": ["federated learning", "differential privacy", "privacy estimation", "Gaussian mechanism", "membership inference"], "tldr": "This paper presents a novel one-shot empirical method for estimating privacy loss in federated learning models trained using differential privacy, demonstrating its effectiveness across various adversarial threat models.", "summary": "The paper introduces a one-shot approach for estimating the privacy loss of deep learning models trained with differential privacy in federated learning contexts, aimed at addressing challenges related to existing auditing techniques that often require extensive retraining or specific knowledge of model and task characteristics. The proposed method involves inserting canary clients that contribute randomly generated updates during the training process, allowing the estimation of privacy loss without needing intermediate model iterates or detailed knowledge about the data. Experimental results on established federated learning benchmarks indicate that the method yields accurate estimates of privacy loss under several adversarial scenarios, showing that modest noise can significantly enhance privacy, thereby providing a practical tool for empirical privacy assessment in real-world federated learning applications."}, {"title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement", "pdf_url": "https://openreview.net/attachment?id=bNt7oajl2a&name=pdf", "topics": ["inductive reasoning", "language models", "hypothesis refinement", "symbolic interpretation", "human cognition"], "tldr": "This paper investigates the inductive reasoning capabilities of language models (LMs) using an iterative hypothesis refinement approach, revealing LMs' strengths as hypothesis proposers but significant limitations in applying their own proposed rules.", "summary": "The paper systematically examines the inductive reasoning capabilities of language models through a novel method called iterative hypothesis refinement, which involves generating, selecting, and refining hypotheses in a manner similar to human reasoning. The study demonstrates that while LMs can propose plausible hypotheses effectively when paired with symbolic interpreters, they often struggle to apply these rules correctly and exhibit brittleness when faced with perturbations in input examples. Through various experimental evaluations across multiple datasets, the authors highlight the paradox of LMs being proficient at proposing hypotheses but puzzlingly inadequate in their application, underscoring gaps between human and LM reasoning processes, and the need for further exploration of their behavior and robustness in inductive tasks."}, {"title": "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning", "pdf_url": "https://openreview.net/attachment?id=o2IEmeLL9r&name=pdf", "topics": ["reinforcement learning", "sample efficiency", "goal-conditioned policy", "hierarchical reinforcement learning", "clustering"], "tldr": "This paper presents PTGM, a pre-training method for goal-based models that enhances sample efficiency and performance in reinforcement learning by utilizing temporal abstractions and behavior regularization.", "summary": "The paper introduces PTGM, a novel approach designed to improve sample efficiency in reinforcement learning (RL) by leveraging pre-training on large, task-agnostic datasets. PTGM pre-trains a goal-conditioned policy and a high-level policy that generates discrete goals to guide the low-level policy, addressing challenges associated with high-dimensional action spaces through a clustering approach. Experimental results in both robotic manipulation (Kitchen) and complex open-world settings (Minecraft) demonstrate PTGM's superior performance compared to various baselines, showcasing its effectiveness in enhancing interpretability and generalization of learned skills while contributing to more efficient exploration in downstream tasks."}, {"title": "Predictive auxiliary objectives in deep RL mimic learning in the brain", "pdf_url": "https://openreview.net/attachment?id=agPpmEgf8C&name=pdf", "topics": ["predictive auxiliary objectives", "deep reinforcement learning", "representation learning", "brain function", "transfer learning"], "tldr": "The paper explores how predictive auxiliary objectives in deep reinforcement learning enhance representation learning and stability in resource-limited environments, mirroring representational changes observed in the brain.", "summary": "This study investigates the effects of predictive auxiliary objectives on representation learning within deep reinforcement learning (RL) systems and their parallels with neural activity in the brain. The authors implemented a deep Q-learning framework that includes a predictive model, allowing them to evaluate how these objectives prevent representational collapse and improve task performance, particularly in resource-constrained architectures. Their findings demonstrate that longer predictive horizons facilitate better representational transfer and show that the representational changes in their RL model closely resemble those seen in various brain regions, particularly the hippocampus and visual cortex. This work suggests a new perspective on the role of the hippocampus in prediction and representation learning, indicating that such systems can benefit other regions even without direct planning functions."}, {"title": "Protein Discovery with Discrete Walk-Jump Sampling", "pdf_url": "https://openreview.net/attachment?id=zMPHKOmQNb&name=pdf", "topics": ["discrete generative models", "protein discovery", "antibodies", "energy-based models", "Walk-Jump Sampling"], "tldr": "This paper introduces Discrete Walk-Jump Sampling (dWJS), a novel method for generating diverse and functional antibody proteins by effectively combining energy-based modeling and Langevin Markov chain Monte Carlo sampling.", "summary": "The authors develop a new approach for generative modeling of antibody proteins named Discrete Walk-Jump Sampling (dWJS), which addresses the challenges associated with discrete sequence generation. They utilize a smoothed energy function to improve training and sampling efficiency, allowing for the effective generation of high-quality samples from a sparse and complex data distribution. The method achieves impressive results in both in silico and in vitro experiments, with 97-100% of generated antibody samples successfully expressed and a 70% binding affinity rate for functional designs. Additionally, the study introduces a Distributional Conformity Score (DCS) to benchmark protein generative models, confirming the robustness and effectiveness of the proposed sampling technique for therapeutic molecule design."}, {"title": "Provable Compositional Generalization for Object-Centric Learning", "pdf_url": "https://openreview.net/attachment?id=7VPTUWkiDQ&name=pdf", "topics": ["compositional generalization", "object-centric learning", "autoencoders", "identifiability theory", "representation learning"], "tldr": "This paper establishes theoretical guarantees for compositional generalization in object-centric learning by leveraging identifiability theory, demonstrating that specific structural assumptions on autoencoders facilitate this capability.", "summary": "The paper investigates how to achieve compositional generalization\u2014 the ability to apply learned concepts to novel combinations\u2014 within object-centric representation learning. By employing identifiability theory, the authors show that autoencoders with a compositional and additive decoder, along with a compositional consistency regularizer, can reliably learn object-centric representations that generalize to out-of-distribution scenarios. Empirical experiments on synthetic image data validate the theoretical framework, indicating that satisfying these assumptions significantly enhances the model's performance in recognizing and reconstructing unseen object combinations. The findings contribute to a deeper understanding of how structured decoding can promote robust machine perception in comparison to human learning capabilities."}, {"title": "Proving Test Set Contamination in Black-Box Language Models", "pdf_url": "https://openreview.net/attachment?id=KS8mIvetg2&name=pdf", "topics": ["test set contamination", "language models", "statistical testing", "exchangeability", "benchmark memorization"], "tldr": "The paper introduces a statistical test for identifying test set contamination in language models based solely on log probability queries, providing provable guarantees of detection without access to training data.", "summary": "This paper addresses concerns regarding dataset contamination in large language models (LLMs) by proposing a novel statistical test that leverages the concept of exchangeability to identify whether a model has been influenced by specific benchmark datasets. The authors demonstrate how to compare the log probability of a dataset's canonical ordering against the probabilities of its shuffled permutations, with significant discrepancies indicating potential contamination. Their test is validated through controlled experiments with intentionally contaminated models and is then applied to audit various publicly available LLMs, finding little evidence of pervasive contamination. The findings emphasize the practicality and effectiveness of the proposed test in auditing language model performance, suggesting that further developments could enhance detection capabilities for lower duplication counts in contaminated datasets."}, {"title": "Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How", "pdf_url": "https://openreview.net/attachment?id=tqh1zdXIra&name=pdf", "topics": ["transfer learning", "hyperparameter optimization", "meta-learning", "pretrained models", "Bayesian optimization"], "tldr": "The paper presents Quick-Tune, a methodology for efficiently selecting and optimizing pretrained models and their hyperparameters for image classification tasks through Bayesian optimization and meta-learning.", "summary": "This paper addresses the challenge of selecting the optimal pretrained model and finetuning hyperparameters from a large pool of options for various image classification tasks. The authors propose Quick-Tune, a Combined Algorithm Selection and Hyperparameter Optimization (CASH) approach that leverages a large-scale meta-dataset created by evaluating over 20k hyperparameter configurations across 24 pretrained models on 87 datasets. The methodology incorporates gray-box optimization, meta-learning for performance prediction, and cost-awareness to speed up the search process. Experimental results demonstrate that Quick-Tune outperforms traditional finetuning methods and state-of-the-art hyperparameter optimization techniques, highlighting its practical utility in enhancing performance while saving time and computational resources."}, {"title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models", "pdf_url": "https://openreview.net/attachment?id=osoWxY8q2E&name=pdf", "topics": ["ReLU", "activation sparsity", "Large Language Models", "inference efficiency", "model optimization"], "tldr": "The paper advocates for the use of ReLU activations in Large Language Models (LLMs) to significantly improve inference efficiency by leveraging activation sparsity with minimal impact on performance.", "summary": "This study investigates the computational challenges of deploying Large Language Models (LLMs) on resource-constrained devices, emphasizing the advantages of using the ReLU activation function over more complex alternatives like GELU or SiLU. The authors demonstrate that ReLU leads to high activation sparsity during inference, which reduces computational load and weight transfer, translating to substantial savings in Floating Point Operations Per Second (FLOPS). The paper outlines a process termed \"relufication,\" where existing pretrained models are fine-tuned to incorporate ReLU activations, showing that this approach quickly restores original performance on various tasks. Additionally, the authors reveal patterns of aggregated sparsity and propose further strategies to enhance inference speed, while introducing the shifted ReLU activation to maximize sparsity without sacrificing performance. Overall, the findings highlight the potential benefits of architectural changes in LLMs for improved efficiency."}, {"title": "Robust agents learn causal world models", "pdf_url": "https://openreview.net/attachment?id=pOoKI3ouv1&name=pdf", "topics": ["Causality", "Robustness", "Generalization", "Transfer Learning", "Decision Making"], "tldr": "The paper establishes that agents capable of robustly adapting to distributional shifts must learn an approximate causal model of the data-generating process to satisfy regret bounds, leading to significant implications for AI robustness and general intelligence.", "summary": "This paper investigates the necessity of causal reasoning for general intelligence by proving that any agent that can achieve low regret in response to various distributional shifts must have learned an approximate causal model of the underlying data-generating process. The authors provide theoretical results demonstrating that robust policies can be derived from these causal models, and they explore implications for transfer learning, causal inference, and the emergence of agent capabilities. The findings highlight important connections between causal representation learning and the ability of agents to handle unseen domain shifts effectively, suggesting that learning causal structures is essential for achieving strong robustness in AI systems."}, {"title": "Self-Alignment with Instruction Backtranslation", "pdf_url": "https://openreview.net/attachment?id=1oijHJBRsT&name=pdf", "topics": ["instruction alignment", "self-training", "language models", "data curation", "backtranslation"], "tldr": "This paper introduces \"instruction backtranslation,\" an innovative self-training method that leverages unlabeled web data to enhance instruction-following capabilities in language models without relying on extensive human annotations.", "summary": "The paper presents a scalable self-training approach called instruction backtranslation, designed to improve instruction-following language models by automatically generating and curating training examples from a web corpus. By initializing with a small set of human-annotated examples, the method iterates through self-augmentation to create candidate instruction-output pairs, followed by quality assessment and selection of high-quality examples. The resultant models, particularly Humpback, demonstrate superior performance against existing non-distilled models on the Alpaca leaderboard, proving that high-quality data curation significantly enhances model capabilities while reducing reliance on human annotations. The findings suggest potential for further improvements through larger unlabeled datasets."}, {"title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "pdf_url": "https://openreview.net/attachment?id=hSyW5go0v8&name=pdf", "topics": ["Self-Reflection", "Retrieval-Augmented Generation", "Large Language Models", "Factual Accuracy", "Critique Mechanism"], "tldr": "This paper introduces SELF-RAG, a framework that enhances the quality and factuality of large language models by enabling on-demand retrieval, generation, and self-critique through innovative reflection tokens, leading to significant performance improvements over existing models.", "summary": "The paper presents SELF-RAG, a novel framework designed to improve the performance of large language models (LLMs) by incorporating a mechanism for on-demand retrieval and self-reflection. Unlike traditional retrieval-augmented generation methods that indiscriminately fetch and integrate external information, SELF-RAG uses special reflection tokens to enable the model to determine when retrieval is necessary, assess the relevance of retrieved passages, and critique its own outputs. The framework was evaluated against various state-of-the-art LLMs and retrieval-augmented models across multiple tasks, demonstrating substantial gains in factual accuracy, citation precision, and overall generation quality. Through detailed experiments, the authors illustrate the effectiveness of SELF-RAG in allowing the model to tailor its responses based on diverse task requirements, thereby addressing common limitations seen in prior approaches."}, {"title": "Small-scale proxies for large-scale Transformer training instabilities", "pdf_url": "https://openreview.net/attachment?id=d8w0pmvXbZ&name=pdf", "topics": ["training instability", "Transformers", "learning rate sensitivity", "model scaling", "mitigation strategies"], "tldr": "This paper explores the reproduction and understanding of training instabilities in large-scale Transformer models through small-scale proxy experiments, revealing effective mitigation strategies and predictive scaling behaviors.", "summary": "The paper investigates training instabilities encountered during large-scale training of Transformer models, specifically focusing on two types of instabilities: attention logit growth and output logit divergence. By replicating these instabilities in smaller models, the authors demonstrate that high learning rates can lead to similar issues even at a reduced scale, allowing for the evaluation of existing mitigation strategies such as qk-layernorm and z-loss. The study introduces the concept of learning rate sensitivity to quantify model performance relative to learning rate across various scales. Additionally, the authors identify predictive behaviors in model characteristics that can signal potential instabilities before they emerge, providing a framework for future research on ensuring stable training in large Transformers. This work allows for the study of instability phenomena without requiring extensive computational resources."}, {"title": "Statistically Optimal K -means Clustering via Nonnegative Low-rank Semidefinite Programming", "pdf_url": "https://openreview.net/attachment?id=v7ZPwoHU1j&name=pdf", "topics": ["K-means clustering", "semidefinite programming", "nonnegative matrix factorization", "low-rank factorization", "statistical optimality."], "tldr": "This paper introduces an efficient nonnegative low-rank algorithm for K-means clustering that combines the scalability of matrix factorization with strong statistical guarantees typically reserved for semidefinite programming.", "summary": "The paper proposes a novel clustering algorithm that addresses the challenges of K-means clustering by utilizing a nonnegative low-rank representation based on a semidefinite programming (SDP) relaxation. It highlights the limitations of current methods like semidefinite programming due to their computational inaccessibility for large datasets, and contrasts this with nonnegative matrix factorization (NMF), which lacks theoretical guarantees. The authors develop a nonnegative low-rank (NLR) approach that maintains the statistical benefits of SDP while achieving the computational efficiency of NMF. Their experiments demonstrate that the NLR algorithm achieves significantly lower mis-clustering errors compared to state-of-the-art methods while being scalable, thus providing a powerful alternative for large-scale clustering problems."}, {"title": "SWE-bench: Can Language Models Resolve Real-world Github Issues?", "pdf_url": "https://openreview.net/attachment?id=VTF8yNQM66&name=pdf", "topics": ["software engineering", "language models", "benchmarks", "GitHub issues", "code generation"], "tldr": "The paper introduces SWE-bench, an evaluation framework for language models that assesses their ability to resolve real-world GitHub issues, revealing that even state-of-the-art models struggle, with the top performer only solving 1.96% of tasks.", "summary": "This paper presents SWE-bench, a novel evaluation framework designed to test the capabilities of language models (LMs) in addressing real-world software engineering problems extracted from GitHub issues. The framework encompasses 2,294 software engineering tasks, each consisting of a codebase and a relevant issue description, requiring models to generate patches that resolve the described problems. To establish a comprehensive evaluation, the authors tested various state-of-the-art LMs, including their own fine-tuned models, and found that these models largely fail to address the complexities of software issues, with the best performing model, Claude 2, resolving only 1.96% of issues. The findings emphasize the need for more challenging benchmarks to guide future developments in LMs that can effectively tackle real-world coding problems."}, {"title": "The mechanistic basis of data dependence and abrupt learning in an in-context classification task", "pdf_url": "https://openreview.net/attachment?id=aN4Jf6Cx69&name=pdf", "topics": ["in-context learning", "transformers", "induction head", "data distribution", "abrupt learning"], "tldr": "This paper investigates the mechanistic basis of in-context learning (ICL) in transformer models, demonstrating that the abrupt emergence of an induction head, influenced by specific data distribution features, differentiates ICL from in-weights learning (IWL).", "summary": "This paper explores the mechanisms underlying in-context learning (ICL) in transformer models, contrasting it with traditional in-weights learning (IWL). By employing a minimal attention-only network trained on simplified datasets, the authors establish that ICL is driven by the sudden formation of an induction head, which competes with IWL. They analyze how distributional properties of the data, such as burstiness and rank-frequency distributions, affect the trade-off between these learning modalities. The results include the construction of a two-parameter model simulating induction head behavior and a phenomenological model that connects the abrupt transitions in learning to a sequential learning process involving nested logits. The findings suggest that intrinsic curricula could enhance ICL in larger language models (LLMs) by facilitating the emergence of more complex learning strategies."}, {"title": "Topological data analysis on noisy quantum computers", "pdf_url": "https://openreview.net/attachment?id=dLrhRIMVmB&name=pdf", "topics": ["Topological Data Analysis", "Quantum Computing", "NISQ-TDA", "Betti Numbers", "Machine Learning"], "tldr": "The paper introduces NISQ-TDA, a quantum algorithm for estimating Betti numbers in topological data analysis, designed for noisy intermediate-scale quantum computers that demonstrates robustness against noise and significant potential for achieving speedups over classical methods.", "summary": "This paper presents NISQ-TDA, a novel quantum machine learning algorithm aimed at estimating Betti numbers from high-dimensional datasets using topological data analysis (TDA) within the framework of Noisy Intermediate-Scale Quantum (NISQ) computing. Unlike traditional methods, NISQ-TDA operates with short circuit depths and avoids the limitations associated with data loading and fault tolerance. The algorithm utilizes an efficient representation of boundary operators, quantum rejection sampling, and stochastic rank estimation to produce estimates for normalized Betti numbers. Empirical results indicate that NISQ-TDA performs well under noise, suggesting strong potential for practical applications of TDA in machine learning and artificial intelligence on NISQ devices. The findings underscore the promise of quantum advantages in handling complex data analysis tasks that are computationally prohibitive for classical approaches."}, {"title": "Towards a statistical theory of data selection under weak supervision", "pdf_url": "https://openreview.net/attachment?id=HhfcNgQn6p&name=pdf", "topics": ["Data selection", "weak supervision", "biased sampling", "statistical learning", "empirical risk minimization"], "tldr": "This paper investigates data selection methods under weak supervision, demonstrating that biased subsampling can outperform traditional unbiased methods in certain scenarios.", "summary": "The paper explores the effectiveness of data selection in machine learning, particularly when working with larger unlabeled datasets and imperfect surrogate models. It proposes a framework where a subset of unlabeled samples is selected for labeling and model training, focusing on the design of selection probabilities that can potentially improve learning outcomes. Through numerical experiments and theoretical analyses under both low- and high-dimensional settings, the authors find that biased data selection methods significantly outperformed traditional unbiased approaches, especially when considering their relationship with the selection fraction. The insights suggest that better utilization of available data, even with weaker surrogates, can lead to lower misclassification rates, challenging common assumptions about unbiased sampling in the field."}, {"title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions", "pdf_url": "https://openreview.net/attachment?id=ekeyCgeRfC&name=pdf", "topics": ["in-context learning", "Transformers", "Boolean functions", "sample efficiency", "Large Language Models"], "tldr": "This paper investigates the in-context learning capabilities of Transformers and Large Language Models (LLMs) across various Boolean function tasks, revealing both their strengths in learning simpler functions and their limitations with more complex ones.", "summary": "The authors explore the phenomenon of in-context learning in Transformers and LLMs by evaluating their performance in learning discrete Boolean functions within a stylized framework. They find that while Transformers can achieve near-optimal performance on simpler tasks, such as conjunctions and disjunctions, their efficacy deteriorates with more complex tasks, like parity functions. The paper also highlights the ability of Transformers to leverage teaching sequences for more sample-efficient learning and shows that certain attention-free models perform similarly to Transformers. Additionally, pretrained LLMs, including popular models like GPT-4 and LLaMA-2, demonstrate competitive performance on tasks outside their training set, indicating an intriguing capacity for in-context learning. The findings underscore both the capabilities and inherent limitations of Transformers and LLMs in implementing learning algorithms, raising questions for future research on improving sample efficiency and understanding pretraining influences."}, {"title": "Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks", "pdf_url": "https://openreview.net/attachment?id=NSVtmmzeRB&name=pdf", "topics": ["geometric generative models", "Bayesian Flow Networks", "molecular geometry", "noise sensitivity", "SE-(3) invariance"], "tldr": "This paper presents Geometric Bayesian Flow Networks (GeoBFN), a novel generative model that effectively represents 3D molecular geometry while overcoming challenges related to noise sensitivity and multi-modality by utilizing SE-(3) invariant density modeling.", "summary": "The paper introduces Geometric Bayesian Flow Networks (GeoBFN), a generative modeling framework specifically designed for 3D molecular geometry. GeoBFN addresses challenges such as noise sensitivity and multi-modality by employing a unified probabilistic modeling approach and incorporating rotational and translational invariance (SE-(3) invariance) in its density functions. The model operates in a differentiable parameter space that enhances compatibility with the inherent noise sensitivity of molecular coordinates. Experimental results demonstrate that GeoBFN outperforms state-of-the-art methods in various benchmarks for molecular generation, achieving high stability rates and enabling sampling with any number of steps, thereby optimizing efficiency and generation quality. The findings indicate GeoBFN's potential for advancing molecular design and discovery."}, {"title": "Unprocessing Seven Years of Algorithmic Fairness", "pdf_url": "https://openreview.net/attachment?id=jr03SfWsBS&name=pdf", "topics": ["algorithmic fairness", "postprocessing", "error rate parity", "unprocessing", "Pareto dominance"], "tldr": "This paper demonstrates that the simple postprocessing method for achieving error rate parity dominates various advanced fairness methods across multiple datasets, suggesting that it can yield optimal fairness-accuracy trade-offs.", "summary": "The authors critically evaluate the effectiveness of algorithmic fairness interventions over the past seven years, particularly focusing on the postprocessing method, which adjusts acceptance thresholds to equalize error rates across demographic groups. Through extensive empirical analysis involving over 11,000 model evaluations across multiple tabular datasets, they establish that postprocessing is Pareto-dominant, meaning it outperforms or matches the accuracy-fairness trade-offs of all other evaluated methods. They address common methodological errors in prior comparisons and introduce the concept of \"unprocessing\" to facilitate fair comparison among models with varying constraints. Their findings underscore the importance of rigorous evaluation standards in the field of algorithmic fairness and suggest that postprocessing remains a compellingly simple yet effective approach to achieving fairness goals."}, {"title": "ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation", "pdf_url": "https://openreview.net/attachment?id=yV6fD7LYkF&name=pdf", "topics": ["uncertainty estimation", "semantic segmentation", "validation framework", "aleatoric uncertainty", "epistemic uncertainty"], "tldr": "This paper presents a systematic framework, ValUES, for the validation of uncertainty estimation methods in semantic segmentation, addressing existing gaps between theory and application by rigorously evaluating uncertainty types, method components, and downstream tasks.", "summary": "The paper identifies significant gaps in the validation of uncertainty estimation methods used in semantic segmentation, particularly regarding the separation and evaluation of aleatoric and epistemic uncertainties. It proposes a comprehensive evaluation framework called ValUES, which includes controlled environments for studying data ambiguities and distribution shifts, systematic ablations of method components, and test beds for various application scenarios. Empirical findings reveal that while uncertainty type separation is feasible in simulated settings, it does not consistently apply to real-world data; moreover, the effectiveness of individual method components, particularly aggregation strategies, varies widely depending on the dataset and task. The study concludes with practical recommendations for researchers and practitioners in the field to enhance the reliability and applicability of uncertainty estimation methods."}, {"title": "Vision Transformers Need Registers", "pdf_url": "https://openreview.net/attachment?id=2dnO3LLiJ1&name=pdf", "topics": ["Vision Transformers", "Artifacts", "Feature Maps", "Register Tokens", "Self-Supervised Learning"], "tldr": "This paper identifies and remediates artifacts in feature maps of Vision Transformers by introducing additional register tokens, improving performance on dense prediction tasks and object discovery methods.", "summary": "The paper investigates artifacts found in the feature maps of Vision Transformers (ViTs), primarily focusing on high-norm tokens that emerge during inference, particularly in low-information background areas of images. The authors propose a solution involving the addition of register tokens to the input sequence, which effectively mitigates these artifacts across various models, including supervised and self-supervised frameworks like DINOv2. Their empirical analysis demonstrates that incorporating register tokens leads to smoother feature maps, enhances performance on dense prediction tasks, and improves results in unsupervised object discovery. The findings indicate that this intervention not only resolves artifact issues but also preserves or even boosts the performance of the models involved."}, {"title": "Wrstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models", "pdf_url": "https://openreview.net/attachment?id=gU58d5QeGv&name=pdf", "topics": ["text-to-image synthesis", "diffusion models", "architecture efficiency", "semantic representation", "computational cost reduction"], "tldr": "The paper introduces Wurstchen, a novel three-stage architecture for text-to-image synthesis that significantly reduces computational costs while achieving competitive image quality.", "summary": "This paper presents Wurstchen, a new architecture designed for large-scale text-to-image synthesis, which combines enhanced performance with a substantial reduction in computational resources required for training and inference. The model employs a three-stage process, using a highly compressed semantic representation to guide diffusion, which drastically lowers training costs to about 24,602 A100-GPU hours compared to 200,000 hours for existing models like Stable Diffusion 2.1. Results demonstrate that Wurstchen not only performs competitively in image quality but also improves inference speed and reduces environmental impact. The authors aim to promote more sustainable practices in generative AI by making their findings and models publicly accessible."}, {"title": "Zipformer: A faster and better encoder for automatic speech recognition", "pdf_url": "https://openreview.net/attachment?id=9WD9KwssyT&name=pdf", "topics": ["Zipformer", "Automatic Speech Recognition", "Neural Networks", "Optimization", "Encoder Models"], "tldr": "The paper introduces Zipformer, a novel and efficient transformer model for automatic speech recognition that outperforms existing models while being faster and less memory-intensive.", "summary": "This paper presents Zipformer, a new encoder model designed for automatic speech recognition (ASR) that enhances both speed and performance compared to existing models, such as the Conformer. Zipformer incorporates a U-Net-like structure for temporal downsampling, a novel block design that reuses attention weights, and introduces BiasNorm for normalization, together with new activation functions (SwooshR and SwooshL). Additionally, a new optimizer called ScaledAdam is proposed to improve convergence by scaling updates according to parameter sizes. Comprehensive experiments on datasets like LibriSpeech, Aishell-1, and WenetSpeech demonstrate Zipformer's state-of-the-art performance, reduced computational requirements, and faster inference times, showcasing its effectiveness over previous models."}];
        let currentPage = 1;
        
        // Function to filter papers based on search input
        function filterPapers(searchText) {
            if (!searchText) return papersData;
            
            searchText = searchText.toLowerCase();
            return papersData.filter(paper => {
                const titleMatch = paper.title.toLowerCase().includes(searchText);
                const topicsMatch = paper.topics.some(topic => 
                    topic.toLowerCase().includes(searchText)
                );
                const tldrMatch = paper.tldr.toLowerCase().includes(searchText);
                const summaryMatch = paper.summary.toLowerCase().includes(searchText);
                
                return titleMatch || topicsMatch || tldrMatch || summaryMatch;
            });
        }
        
        // Function to create paper card HTML
        function createPaperCard(paper) {
            const showTopics = document.getElementById('showTopics').checked;
            const showTldr = document.getElementById('showTldr').checked;
            const showSummary = document.getElementById('showSummary').checked;

            const topicsHtml = (showTopics && paper.topics.length > 0)
                ? `<div class="mb-3">
                     <div class="d-flex gap-2 flex-wrap">
                       ${paper.topics.map(topic => `<span class="badge text-bg-info">${topic}</span>`).join('')}
                     </div>
                   </div>`
                : '';
                
            const tldrHtml = (showTldr && paper.tldr)
                ? `<div class="mb-3">
                     <h3 class="h5">TL;DR</h3>
                     <p class="card-text">${paper.tldr}</p>
                   </div>`
                : '';
                
            const summaryHtml = (showSummary && paper.summary)
                ? `<div class="mb-3">
                     <h3 class="h5">Summary</h3>
                     <p class="card-text">${paper.summary}</p>
                   </div>`
                : '';
                
            const urlHtml = paper.pdf_url
                ? `<p class="card-text"><a href="${paper.pdf_url}" class="btn btn-outline-primary btn-sm">Download Paper</a></p>`
                : '';
                
            return `
                <div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
                    <div class="card shadow-sm">
                        <div class="card-body">
                            <h3 class="card-title h4">${paper.title}</h3>
                            ${topicsHtml}
                            ${tldrHtml}
                            ${summaryHtml}
                            ${urlHtml}
                        </div>
                    </div>
                </div>
            `;
        }

        // Function to create pagination controls
        function createPagination(totalItems) {
            const itemsPerPage = parseInt(document.getElementById('itemsPerPage').value);
            const totalPages = Math.ceil(totalItems / itemsPerPage);
            const pagination = document.getElementById('pagination');
            
            let paginationHtml = '';
            
            // Previous button
            paginationHtml += `
                <li class="page-item ${currentPage === 1 ? 'disabled' : ''}">
                    <a class="page-link" href="#" data-page="${currentPage - 1}">Previous</a>
                </li>
            `;
            
            // Page numbers with ellipsis
            const maxVisiblePages = 5;  // Adjust this number to show more or fewer pages
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust startPage if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            // First page and ellipsis
            if (startPage > 1) {
                paginationHtml += `
                    <li class="page-item">
                        <a class="page-link" href="#" data-page="1">1</a>
                    </li>
                `;
                if (startPage > 2) {
                    paginationHtml += `
                        <li class="page-item disabled">
                            <span class="page-link">...</span>
                        </li>
                    `;
                }
            }
            
            // Visible pages
            for (let i = startPage; i <= endPage; i++) {
                paginationHtml += `
                    <li class="page-item ${currentPage === i ? 'active' : ''}">
                        <a class="page-link" href="#" data-page="${i}">${i}</a>
                    </li>
                `;
            }
            
            // Last page and ellipsis
            if (endPage < totalPages) {
                if (endPage < totalPages - 1) {
                    paginationHtml += `
                        <li class="page-item disabled">
                            <span class="page-link">...</span>
                        </li>
                    `;
                }
                paginationHtml += `
                    <li class="page-item">
                        <a class="page-link" href="#" data-page="${totalPages}">${totalPages}</a>
                    </li>
                `;
            }
            
            // Next button
            paginationHtml += `
                <li class="page-item ${currentPage === totalPages ? 'disabled' : ''}">
                    <a class="page-link" href="#" data-page="${currentPage + 1}">Next</a>
                </li>
            `;
            
            pagination.innerHTML = paginationHtml;
            
            // Add click handlers
            pagination.querySelectorAll('.page-link').forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const newPage = parseInt(e.target.dataset.page);
                    if (!isNaN(newPage) && newPage >= 1 && newPage <= totalPages) {
                        currentPage = newPage;
                        renderPapers();
                    }
                });
            });
        }

        // Render papers with pagination
        function renderPapers() {
            const searchText = document.getElementById('searchInput').value;
            const filteredPapers = filterPapers(searchText);
            const itemsPerPage = parseInt(document.getElementById('itemsPerPage').value);
            
            // Calculate pagination
            const startIndex = (currentPage - 1) * itemsPerPage;
            const endIndex = startIndex + itemsPerPage;
            const paginatedPapers = filteredPapers.slice(startIndex, endIndex);
            
            // Render papers
            const container = document.getElementById('papers-container');
            container.innerHTML = paginatedPapers.map(createPaperCard).join('');
            
            // Create pagination controls
            createPagination(filteredPapers.length);
            
            // Initialize Masonry layout
            new Masonry(container, {
                percentPosition: true
            });
        }

        // Add event listeners
        document.getElementById('searchInput').addEventListener('input', () => {
            currentPage = 1;  // Reset to first page on search
            renderPapers();
        });
        document.getElementById('itemsPerPage').addEventListener('change', () => {
            currentPage = 1;  // Reset to first page when changing items per page
            renderPapers();
        });
        document.getElementById('showTopics').addEventListener('change', renderPapers);
        document.getElementById('showTldr').addEventListener('change', renderPapers);
        document.getElementById('showSummary').addEventListener('change', renderPapers);

        // Render papers when page loads
        document.addEventListener('DOMContentLoaded', renderPapers);
    </script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
