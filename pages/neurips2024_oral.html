
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>PubSummarizer - NeurIPS 2024 Oral Papers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <style>
        .filter-controls {
            margin-bottom: 20px;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 8px;
        }
        .pagination-controls {
            margin: 20px 0;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 8px;
        }
        @media (max-width: 991px) {
            .search-box {
                margin-bottom: 15px;
            }
        }
    </style>
</head>
<body>
    <div class="container py-4">
        <h1 class="mb-4">NeurIPS 2024 Oral Papers</h1>
        <p class="text-muted"><em>Generated by <a href="https://github.com/Insights-Ac/PaperBriefing">PaperBriefing</a></em></p>
        
        <div class="filter-controls">
            <div class="d-flex flex-md-row flex-column gap-1">
                <div class="flex-grow-1">
                    <input type="text" class="form-control form-control-sm search-box" id="searchInput" 
                           placeholder="Search in titles, topics, TL;DR, and summaries...">
                </div>
                <div class="btn-group btn-group-sm" role="group" aria-label="Section toggles">
                    <input type="checkbox" class="btn-check" id="showTopics" checked autocomplete="off">
                    <label class="btn btn-outline-primary" for="showTopics">Topics</label>

                    <input type="checkbox" class="btn-check" id="showTldr" checked autocomplete="off">
                    <label class="btn btn-outline-primary" for="showTldr">TL;DR</label>

                    <input type="checkbox" class="btn-check" id="showSummary" checked autocomplete="off">
                    <label class="btn btn-outline-primary" for="showSummary">Summary</label>
                </div>
            </div>
        </div>

        <div class="pagination-controls">
            <div class="d-flex flex-wrap justify-content-between align-items-center gap-1">
                <div class="d-flex align-items-center gap-2">
                    <label for="itemsPerPage">Items per page:</label>
                    <select class="form-select form-select-sm" id="itemsPerPage" style="width: auto;">
                        <option value="6">6</option>
                        <option value="12" selected>12</option>
                        <option value="24">24</option>
                        <option value="48">48</option>
                    </select>
                </div>
                <nav aria-label="Page navigation">
                    <ul class="pagination pagination-sm mb-0" id="pagination"></ul>
                </nav>
            </div>
        </div>

        <div id="papers-container" class="row" data-masonry='{"percentPosition": true }'></div>
    </div>

    <script>
        // Store papers data and pagination state
        const papersData = [{"title": "Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures", "pdf_url": "https://openreview.net/attachment?id=ge8GZn8Gtu&name=pdf", "topics": ["clustering", "Gaussian Mixture Models", "anisotropic covariance", "Lloyd's algorithm", "minimax bounds"], "tldr": "The paper presents new theoretical and algorithmic advancements in clustering using Gaussian Mixture Models with anisotropic covariance structures, achieving minimax optimality through an adjusted version of Lloyd's algorithm.", "summary": "This paper explores clustering under anisotropic Gaussian Mixture Models (GMMs) where clusters have unknown and non-identical covariance matrices, addressing two scenarios: homogeneous and heterogeneous covariance. The authors derive minimax lower bounds for clustering performance that highlight the significant role of covariance structures compared to isotropic cases. They introduce a variant of Lloyd's algorithm that iteratively estimates both cluster centers and covariance matrices, demonstrating through theoretical proofs that this adjusted algorithm converges optimally within logarithmic iterations. The paper includes numerical comparisons to established methods, showing improved performance of the proposed algorithm under various settings, thus bridging theoretical optimality and practical computational efficiency in clustering tasks."}, {"title": "Aligner: Efficient Alignment by Learning to Correct", "pdf_url": "https://openreview.net/attachment?id=kq166jACVP&name=pdf", "topics": ["alignment", "large language models", "efficiency", "residual learning", "human feedback"], "tldr": "This paper presents Aligner, a novel lightweight and model-agnostic alignment method that significantly improves the performance of large language models (LLMs) through efficient residual learning based on preferred and dispreferred responses.", "summary": "The paper introduces Aligner, a new paradigm for aligning large language models (LLMs) with human intentions by learning correctional residuals between preferred and unpreferred answers without extensive computational resources or complex frameworks like reinforcement learning. Aligner is designed to be a plug-and-play module that can be easily integrated into various existing LLMs, improving metrics of helpfulness, harmlessness, and honesty across multiple models. Experimental results demonstrate significant performance enhancements, with Aligner -7B improving the helpfulness and harmlessness of LLMs such as GPT-4 by substantial percentages. The findings imply that Aligner can facilitate the rapid evolution of safety standards in language models, making advanced alignment methods more accessible and resource-efficient."}, {"title": "Bayesian-guided Label Mapping for Visual Reprogramming", "pdf_url": "https://openreview.net/attachment?id=135eKqDoRR&name=pdf", "topics": ["Visual Reprogramming", "Label Mapping", "Bayesian Inference", "Pretrained Models", "Downstream Tasks"], "tldr": "This paper introduces Bayesian-guided Label Mapping (BLM) for visual reprogramming, enhancing label correspondence between pretrained and downstream tasks to achieve superior classification performance.", "summary": "The paper addresses the limitations of existing label mapping methods in visual reprogramming (VR), which transform outputs of pretrained models to align with distinct downstream label spaces. The authors propose a Bayesian-guided Label Mapping (BLM) method that creates a probabilistic label mapping matrix, capturing the complex relationships between pretrained and downstream labels through Bayesian conditional probabilities. This approach allows for flexible many-to-many mappings, as demonstrated through extensive experiments on various datasets using pretrained vision models and vision-language models. The results indicate that BLM and its extension, BLM+, significantly outperform traditional one-to-one mapping strategies, providing both improved accuracy and insightful interpretations of label connections, thus highlighting the effectiveness of the proposed methods in leveraging pretrained model knowledge for diverse tasks."}, {"title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs", "pdf_url": "https://openreview.net/attachment?id=Vi8AepAXGy&name=pdf", "topics": ["multimodal LLMs", "vision-centric design", "visual representation learning", "instruction tuning", "benchmark evaluation"], "tldr": "The paper introduces Cambrian-1, a family of multimodal large language models that achieve state-of-the-art performance through a vision-centric approach, incorporating innovative techniques for visual representation learning and a new vision-centric benchmark called CV-Bench.", "summary": "This paper presents Cambrian-1, a set of multimodal large language models (MLLMs) designed to address limitations in current visual representation methods by employing a vision-centric approach. The authors evaluate over 20 vision encoders, proposing the Spatial Vision Aggregator (SVA) to effectively integrate visual features with LLMs while reducing token counts. They also introduce a new benchmark, CV-Bench, to assess the visual grounding capabilities of MLLMs more accurately. The study finds that careful design of visual components and instruction-tuning data significantly enhances model performance, achieving superior results on various multimodal tasks. The authors provide comprehensive resources, including model weights, code, datasets, and tuning strategies, which aim to foster further research in multimodal systems and visual representation learning."}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "pdf_url": "https://openreview.net/attachment?id=TFZlFRl9Ks&name=pdf", "topics": ["3D Reconstruction", "Multi-View Diffusion Models", "Text-to-Image", "Novel View Synthesis", "Photogrammetry"], "tldr": "CAT3D is a novel method that enables efficient 3D scene creation from one or more images using a multi-view diffusion model, significantly reducing the time and input requirements compared to existing techniques.", "summary": "This paper presents CAT3D, a method for creating 3D scenes from various input images by simulating the image capture process through a multi-view diffusion model. CAT3D transforms the key challenge of insufficient observations into a generation problem by creating consistent novel views of a scene from any number of input images, which can then be processed by robust 3D reconstruction techniques. The model showcases rapid production of high-quality 3D representations, achieving results in as little as one minute while outperforming current benchmarks for single-image and few-view reconstructions. The authors validate CAT3D across a range of input scenarios, highlighting its advantages in efficiency and visualization quality despite certain limitations in handling diverse camera intrinsics and achieving consistent view generation."}, {"title": "Convolutional Differentiable Logic Gate Networks", "pdf_url": "https://openreview.net/attachment?id=4bKEFyUHT4&name=pdf", "topics": ["differentiable logic gates", "convolutional networks", "efficient inference", "CIFAR-10", "hardware optimization"], "tldr": "The paper introduces convolutional differentiable logic gate networks that enhance efficiency in inference while achieving superior accuracy on the CIFAR-10 dataset with significantly fewer logic gates than state-of-the-art approaches.", "summary": "This paper presents convolutional differentiable logic gate networks (LogicTreeNet) as a novel approach to fast and efficient machine learning inference, building upon previous work in differentiable logic gate networks to incorporate deep logic gate tree convolutions, logical OR pooling, and residual initializations. The proposed architecture demonstrates the ability to learn effective representations that capture spatial patterns in image data while minimizing the number of logic gates required for achieving competitive accuracy, reporting an accuracy of 86.29% on the CIFAR-10 dataset using only 61 million gates. This improves upon the state-of-the-art results, reducing the gate count by a factor of 29. The efficiency gains are further validated through hardware implementation, achieving drastically faster inference times on FPGA, highlighting the practical applicability of this method for real-time and embedded applications."}, {"title": "Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions", "pdf_url": "https://openreview.net/attachment?id=bCMpdaQCNW&name=pdf", "topics": ["humor", "comics", "juxtaposition", "AI comprehension", "Y ESBUT benchmark"], "tldr": "This paper introduces the Y ESBUT benchmark to assess AI models' understanding of humor in comics through juxtaposed contradictory narratives, revealing significant gaps in their comprehension compared to human performance.", "summary": "The research investigates the challenges that large multimodal language models face in understanding human humor expressed through comics featuring juxtaposed narratives that create humorous contradictions. It introduces the Y ESBUT benchmark, comprising tasks that range from basic literal comprehension to deeper narrative reasoning, enabling a comprehensive evaluation of AI's capability to interpret these comics. Experiments conducted on various state-of-the-art vision-language models indicate that while commercial models generally outperform open-source alternatives, they still lag significantly behind human performance in grasping the nuances of comic narratives. The findings highlight the limitations of current AI models and suggest areas for improving their semantic understanding of human creative expressions."}, {"title": "DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices", "pdf_url": "https://openreview.net/attachment?id=Pezt0xttae&name=pdf", "topics": ["Federated Learning", "Domain Adaptation", "Model Pruning", "Edge Devices", "System Heterogeneity"], "tldr": "The paper presents DapperFL, a federated learning framework that integrates Model Fusion Pruning and Domain Adaptive Regularization to enhance model performance and reduce resource consumption on heterogeneous edge devices facing domain shifts.", "summary": "DapperFL addresses the challenges of system heterogeneity and domain shifts in federated learning environments by introducing a Model Fusion Pruning (MFP) module, which optimizes local models for edge devices by incorporating both local and global knowledge, and a Domain Adaptive Regularization (DAR) module, which encourages robust representation learning across domains. The framework demonstrates superior performance, achieving improved accuracy on benchmark datasets compared to state-of-the-art federated learning frameworks, while also significantly reducing model size by up to 80%. Experimental results validate the efficacy of DapperFL in heterogeneous settings, making it a promising solution for real-world applications in edge computing."}, {"title": "Decompose, Analyze and Rethink: Solving Intricate Problems with Human-like Reasoning Cycle", "pdf_url": "https://openreview.net/attachment?id=NPKZF1WDjZ&name=pdf", "topics": ["reasoning framework", "large language models", "Decomposing", "logical coherence", "error correction."], "tldr": "This paper introduces DeAR (Decompose-Analyze-Rethink), a framework that enhances reasoning capabilities in large language models by systematically decomposing questions, analyzing sub-questions, and iteratively rethinking rationales for greater accuracy.", "summary": "The paper presents DeAR, a novel reasoning framework designed for complex problem-solving in large language models (LLMs) such as GPT-3.5 and LLaMA2. DeAR utilizes a top-down reasoning process that decomposes intricate questions into simpler sub-questions, followed by generating and self-checking rationales, and revising previous rationales based on new insights. Experiments conducted on reasoning benchmarks like ScienceQA, StrategyQA, and GSM8K show that DeAR significantly outperforms existing methods, including Tree-of-Thoughts and Graph-of-Thoughts, in terms of logical coherence and accuracy while maintaining superior efficiency in reasoning tasks. The findings indicate that DeAR effectively reduces errors and enhances the interpretability of the reasoning process, suggesting substantial implications for advancing artificial intelligence in reasoning and cognitive tasks."}, {"title": "DenoiseRep: Denoising Model for Representation Learning", "pdf_url": "https://openreview.net/attachment?id=OycU0bAus6&name=pdf", "topics": ["Denoising", "Representation Learning", "Discriminative Models", "Feature Extraction", "Person Re-Identification"], "tldr": "The paper introduces DenoiseRep, a novel model that integrates denoising processes into representation learning to enhance feature discrimination in various vision tasks.", "summary": "This paper presents DenoiseRep, a method that innovatively combines denoising techniques with representation learning, particularly in discriminative tasks such as image classification and person re-identification. By treating each embedding layer in neural network architectures as a denoising layer, DenoiseRep allows for recursive denoising of features while maintaining computational efficiency through a parameter fusion algorithm, eliminating additional latency. Extensive experiments across multiple datasets, including person re-identification and large-scale image classification, demonstrate that DenoiseRep significantly improves feature discrimination and model performance, proving effective in both label-free and label-augmented scenarios. The results indicate the strong generalization capability of DenoiseRep across various architectures, including CNNs and Transformers."}, {"title": "Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation", "pdf_url": "https://openreview.net/attachment?id=cFqAANINgW&name=pdf", "topics": ["code generation", "divide-and-conquer", "functional consensus", "large language models", "programming tasks"], "tldr": "FUNCODER is a novel code generation framework that combines a divide-and-conquer strategy with functional consensus, significantly improving the performance of programming tasks in complex requirements.", "summary": "The paper introduces FUNCODER, a code generation framework designed to enhance the capabilities of large language models (LLMs) in managing complex programming tasks. FUNCODER employs a divide-and-conquer strategy to break down problems into smaller sub-functions, facilitating iterative coding and reducing complexity. It incorporates a functional consensus mechanism that evaluates multiple function implementations to select the most accurate one, thereby minimizing error propagation. Experiments demonstrate that FUNCODER outperforms state-of-the-art methods across various benchmarks, yielding a notable average improvement in problem-solving tasks and showcasing its robustness even with smaller models. The findings suggest that FUNCODER effectively addresses challenges in code generation, making it a promising tool for future advancements in programming assistance."}, {"title": "Do Finetti: On Causal Effects for Exchangeable Data", "pdf_url": "https://openreview.net/attachment?id=4rCZeCZAON&name=pdf", "topics": ["Causality", "Exchangeable Data", "ICM Generative Processes", "Causal Effect Estimation", "Do-Finetti Algorithm"], "tldr": "This paper presents a framework for estimating causal effects in exchangeable data by developing a Do-Finetti algorithm that addresses causal discovery and effect estimation simultaneously.", "summary": "The paper explores causal effect estimation in contexts where data is not independent and identically distributed (i.i.d.) but rather exchangeable, underpinned by independent causal mechanisms (ICM). It introduces a generalized framework for handling causal inference in ICM generative processes, providing a truncated factorization formula that enables the identification and estimation of causal effects. The authors illustrate their methods using a causal Plya urn model, demonstrating how interventional effects propagate in an exchangeable setting. The paper culminates in the development of the Do-Finetti algorithm, which allows for the simultaneous identification of causal graphs and estimation of causal effects from multi-environment data, demonstrating improved performance compared to traditional i.i.d. methods."}, {"title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs", "pdf_url": "https://openreview.net/attachment?id=mp8u2Pcmqz&name=pdf", "topics": ["quantization", "large language models", "outliers", "rotation", "permutation"], "tldr": "This paper introduces DuQuant, an innovative method for quantizing large language models that effectively redistributes both normal and massive activations outliers, resulting in better performance in low-bit quantization scenarios.", "summary": "The paper addresses significant challenges in quantizing large language models (LLMs) due to activation outliers that hinder efficient low-bit representation. It presents DuQuant, a novel approach that employs rotation and permutation transformations to redistribute outlier activations among channels, thus mitigating their impact. The method consists of a greedy algorithm to construct a rotation matrix for local outlier redistribution and a zigzag permutation to balance outlier distributions globally. Experimental results demonstrate that DuQuant significantly outperforms state-of-the-art quantization techniques across various LLMs and tasks, achieving improvements in perplexity and accuracy while reducing memory usage and improving inference speed. The findings highlight DuQuant's effectiveness in enhancing the deployment of quantized LLMs in resource-constrained environments."}, {"title": "E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection", "pdf_url": "https://openreview.net/attachment?id=47loYmzxep&name=pdf", "topics": ["Multimodal Fusion", "Object Detection", "End-to-End Learning", "Gradient Matrix Task-Alignment", "Autonomous Driving"], "tldr": "E2E-MFD introduces a novel end-to-end framework for multimodal fusion detection that efficiently integrates image fusion and object detection through synchronous optimization, achieving superior performance on public datasets.", "summary": "This paper presents E2E-MFD, an innovative end-to-end algorithm aimed at advancing multimodal image fusion and object detection, predominantly for applications in autonomous driving. The method simplifies the traditionally complex training processes associated with multimodal fusion, allowing for synchronous joint optimization that enhances results across both tasks. E2E-MFD employs an Object-Region-Pixel Phylogenetic Tree for effective feature extraction and a Coarse-to-Fine Diffusion Process for object detection, alongside a novel Gradient Matrix Task-Alignment to ensure an optimal parameter balance. Extensive experiments on various public datasets demonstrate that E2E-MFD significantly outperforms state-of-the-art approaches, showcasing improved image fusion quality and detection accuracy, thus highlighting its potential for practical applications in challenging environments."}, {"title": "Enhancing Preference-based Linear Bandits via Human Response Time", "pdf_url": "https://openreview.net/attachment?id=aIPwlkdOut&name=pdf", "topics": ["Preference learning", "linear bandits", "human response time", "utility estimation", "EZ-diffusion model"], "tldr": "This paper proposes a method to enhance preference-based linear bandit algorithms by incorporating human response times to improve utility estimations from binary choice data.", "summary": "The paper addresses the limitations of traditional interactive preference learning systems that rely solely on human binary choices, which do not provide enough information on preference strength. It introduces an efficient method utilizing human response times, which correlate inversely with preference strength, alongside choices to derive a more informative linear utility estimator based on the EZ-diffusion model. Through theoretical and empirical comparisons, the authors demonstrate that incorporating response times significantly accelerates preference learning and reduces identification errors in fixed-budget best-arm identification scenarios. The results, validated through simulations on real-world datasets, highlight the potential for improved user preference elicitation in applications such as recommender systems and assistive technologies."}, {"title": "Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering", "pdf_url": "https://openreview.net/attachment?id=R8SolCx62K&name=pdf", "topics": ["Graph Contrastive Learning", "Representation Scattering", "Scattering Graph Representation Learning", "Topology-based Constraint Mechanism", "GNN"], "tldr": "The paper introduces Scattering Graph Representation Learning (SGRL), a novel framework for graph contrastive learning that leverages a representation scattering mechanism to enhance the performance of GNNs without the need for manual annotations.", "summary": "This paper explores the common mechanism of representation scattering among three established frameworks in Graph Contrastive Learning (GCL): node discrimination, group discrimination, and bootstrapping schemes. It presents Scattering Graph Representation Learning (SGRL), which utilizes a Representation Scattering Mechanism (RSM) to encourage diverse node representations and a Topology-based Constraint Mechanism (TCM) to account for the interconnected nature of graphs. Extensive experiments demonstrate that SGRL outperforms existing methods across multiple downstream tasks on various benchmark datasets, confirming the significance of representation scattering for enhancing generalization and robustness in GNN training. The findings provide a structured framework for advancing graph representation learning, with implications for broader applications in fields requiring efficient processing of graph data."}, {"title": "Flipped Classroom: Aligning Teacher Attention with Student in Generalized Category Discovery", "pdf_url": "https://openreview.net/attachment?id=C4NbtYnyQg&name=pdf", "topics": ["Generalized Category Discovery", "Teacher-Student Framework", "Attention Alignment", "Semi-Supervised Learning", "Energy Perspective"], "tldr": "This paper introduces FlipClass, an innovative method for improving learning synchronization in generalized category discovery by dynamically aligning teacher and student attention.", "summary": "The paper addresses the challenges of traditional teacher-student frameworks in open-world generalized category discovery (GCD), particularly the issues arising from inconsistent learning between teacher and student due to the introduction of new classes. It proposes FlipClass, a method that dynamically adapts teacher attention based on student feedback to ensure synchronized learning, thus enhancing representation consistency across both old and new classes. Experimental results demonstrate that FlipClass significantly outperforms state-of-the-art methods on various benchmarks, establishing new standards in the field and highlighting its potential for improving model adaptability in real-world settings."}, {"title": "Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure", "pdf_url": "https://openreview.net/attachment?id=m1a4CrRJR7&name=pdf", "topics": ["recommender systems", "generalization error", "tree structure", "theoretical analysis", "empirical validation"], "tldr": "This paper analyzes the generalization error bounds of two-stage recommender systems with tree structures, demonstrating that increased branches in the retriever and harmonized distributions between training and inference can enhance generalization performance.", "summary": "The paper investigates the generalization error of two-stage recommender systems that incorporate tree structures, consisting of an efficient retriever and a more precise ranker. Using an error decomposition framework and Rademacher complexity, the authors derive upper bounds for the generalization error of various tree-based retrievers and ranker models under shifted training distributions. They demonstrate that increasing the number of branches in the tree structure improves generalization capabilities and that aligning training and inference distributions enhances model performance. Theoretical insights are supported by empirical experiments on real-world datasets, reinforcing the findings and highlighting strategies for optimizing two-stage recommender systems."}, {"title": "Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework", "pdf_url": "https://openreview.net/attachment?id=tnh4LK72yj&name=pdf", "topics": ["Spatiotemporal learning", "Multi-task learning", "Urban intelligence", "Continuous learning", "Data integration"], "tldr": "The paper introduces a Continuous Multi-task Spatio-Temporal learning framework (CMuST) that enhances urban intelligence by integrating multiple tasks and data sources to overcome limitations of traditional task-specific models.", "summary": "The paper presents CMuST, a framework designed to address the shortcomings of traditional spatiotemporal learning models that focus on isolated tasks, which often fail to generalize across dynamic urban environments. By utilizing a Multi-dimensional Spatio-Temporal Interaction Network (MSTI) that facilitates interactions between various data types and a Rolling Adaptation (RoAda) training scheme, CMuST effectively models relationships across multiple urban tasks, ensuring continuous learning and efficient adaptation. The authors empirically validate the effectiveness of CMuST through extensive experiments on datasets from three cities, showing significant performance improvements in prediction accuracy on both mainstream and new tasks. The findings highlight the framework's potential in fostering collective intelligence within urban systems, making it a significant contribution to the field of spatiotemporal learning."}, {"title": "GIC: Gaussian-Informed Continuum for Physical Property Identification and Simulation", "pdf_url": "https://openreview.net/attachment?id=SSCtCq2MH2&name=pdf", "topics": ["Gaussian representation", "physical property estimation", "dynamic reconstruction", "simulation", "object identification"], "tldr": "This paper presents a novel hybrid framework leveraging 3D Gaussian representations for accurate estimation of physical properties through visual observations.", "summary": "The paper introduces a hybrid framework called the Gaussian-Informed Continuum (GIC) that addresses the challenge of estimating physical properties of objects using visual observations. The proposed method incorporates a dynamic 3D Gaussian network for precise scene reconstruction and utilizes a coarse-to-fine filling strategy to generate density fields representing object shapes. By integrating 2D shape masks during simulations, the GIC effectively enhances the physical property estimation process. Extensive experiments demonstrate that the GIC outperforms existing methods across various benchmarks and applications, including real-world scenarios like robotic manipulation, thereby showcasing its practical utility and robustness in system identification tasks."}, {"title": "Graph Diffusion Transformers for Multi-Conditional Molecular Generation", "pdf_url": "https://openreview.net/attachment?id=cfrDLD1wfO&name=pdf", "topics": ["molecular generation", "diffusion models", "multi-conditional design", "Graph Diffusion Transformer", "polymer design"], "tldr": "The paper introduces Graph Diffusion Transformer (Graph DiT), an innovative model for multi-conditional molecular generation that effectively integrates numerous property constraints and demonstrates superior performance in generating polymers and small molecules.", "summary": "This paper presents the Graph Diffusion Transformer (Graph DiT), designed to enhance inverse molecular design through multi-conditional constraints, which is particularly useful in drug and material discovery. Graph DiT employs a Transformer-based denoising approach that combines a condition encoder for learning numerical and categorical property representations with a unique graph-dependent noise model to accurately estimate noise in molecular graphs. Extensive validation demonstrates Graph DiT's superiority over existing methods across various metrics for molecular property control and generative accuracy. The model's effectiveness in polymer design for gas separation tasks, supported by domain expert feedback, underscores its practical applicability and promise in advancing molecular design strategies."}, {"title": "Guiding a Diffusion Model with a Bad Version of Itself", "pdf_url": "https://openreview.net/attachment?id=bg6fVPVs3s&name=pdf", "topics": ["diffusion models", "image generation", "classifier-free guidance", "autoguidance", "conditional sampling"], "tldr": "The paper introduces a novel method called autoguidance that effectively utilizes a smaller, less-trained version of a diffusion model to enhance image quality while maintaining diversity in generated images.", "summary": "This paper addresses challenges in image-generating diffusion models, particularly in achieving optimal image quality and alignment with given conditions while retaining variation in outputs. The authors critique the standard classifier-free guidance (CFG) method, which often leads to reduced variation at the cost of quality. They propose autoguidance, a technique that employs a less capable version of the original model for guidance during image generation. This approach results in improved image quality (achieving state-of-the-art FID scores) without sacrificing diversity. The study validates autoguidance using various experiments on ImageNet datasets, illustrating its effectiveness over existing methods and making it applicable to both conditional and unconditional image synthesis tasks."}, {"title": "Human Expertise in Algorithmic Prediction", "pdf_url": "https://openreview.net/attachment?id=wpGJ2AX6SZ&name=pdf", "topics": ["human-AI collaboration", "algorithmic prediction", "human expertise", "medical diagnostics", "multicalibration"], "tldr": "This paper introduces a novel framework for incorporating human expertise into algorithmic predictions, demonstrating that expert feedback can significantly enhance predictive performance in specific instances where algorithms falter.", "summary": "The paper addresses the challenge of integrating human judgment into algorithmic predictions, particularly in high-stakes scenarios such as medical diagnostics. It proposes a framework that identifies \"algorithmically indistinguishable\" inputs\u2014instances where algorithms lack predictive power but experts can still provide valuable insights. By employing multicalibration techniques, the authors demonstrate how to effectively incorporate human feedback into predictive algorithms, leading to significant performance improvements on specific subsets of data. Empirical studies, including a chest X-ray classification task, show that while algorithms may generally outperform human experts, there exists a considerable population subgroup (approximately 30%) where human judgment enhances predictions. The findings advocate for improved human-AI collaboration to leverage both human insight and algorithmic efficiency, suggesting implications for healthcare and decision-making technologies."}, {"title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "pdf_url": "https://openreview.net/attachment?id=qEpi8uWX3N&name=pdf", "topics": ["HydraLoRA", "Parameter-Efficient Fine-Tuning", "Large Language Models", "Asymmetric Architecture", "Mixture-of-Experts"], "tldr": "This paper introduces HydraLoRA, a novel asymmetric fine-tuning architecture for Large Language Models that improves task adaptation efficiency by mitigating parameter interference through shared and distinct matrices.", "summary": "The paper presents HydraLoRA, an innovative framework aimed at enhancing the efficiency of fine-tuning Large Language Models (LLMs) through an asymmetric architecture. Traditional Parameter-Efficient Fine-Tuning (PEFT) methods, like LoRA, often suffer from performance deterioration in complex datasets due to task interference. To tackle this, HydraLoRA employs a shared matrix for common knowledge and multiple distinct matrices for specific tasks, automatically identifying intrinsic components during training using a Mixture-of-Experts (MoE) setup. Experimental results demonstrate that HydraLoRA outperforms existing PEFT techniques across various domains, achieving better performance with fewer parameters and without relying on domain expertise, ultimately providing a more efficient means of adapting LLMs to diverse tasks."}, {"title": "Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments", "pdf_url": "https://openreview.net/attachment?id=S2P6KPLtm8&name=pdf", "topics": ["Mendelian randomization", "bi-directional causality", "instrumental variables", "causal inference", "observational data"], "tldr": "The paper introduces a novel method to identify valid instrumental variables and estimate causal effects in bi-directional Mendelian randomization models despite the presence of invalid instruments and unmeasured confounding.", "summary": "This paper addresses the complex problem of estimating causal effects in bi-directional Mendelian randomization (MR) using observational data, where some instrumental variables may be invalid, and unmeasured confounding exists. The authors present necessary and sufficient conditions for the identification of valid instrumental variables (IVs) in bi-directional MR models and develop a cluster fusion-like algorithm, termed PReBiM, to discover these valid sets and estimate causal effects. The theoretical correctness of the proposed method is established, and extensive experiments demonstrate its efficacy in accurately estimating causal effects compared to traditional methods in both one-directional and bi-directional contexts. The findings highlight the potential for deeper insights into causal relationships in various fields, including genetics and epidemiology."}, {"title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "pdf_url": "https://openreview.net/attachment?id=tQukGCDaNT&name=pdf", "topics": ["diffusion models", "distillation", "image synthesis", "GAN", "text-to-image"], "tldr": "This paper introduces DMD2, an advanced method for fast image synthesis that improves upon Distribution Matching Distillation (DMD) by eliminating costly regression losses and integrating GAN approaches, achieving state-of-the-art results in one-step image generation.", "summary": "The paper presents DMD2, an enhanced technique for converting complex diffusion models into efficient, few-step image generators. By removing the regression loss required in previous DMD approaches, the authors mitigate the heavy computational burden associated with dataset construction while maintaining training stability through a Two Time-scale Update Rule and a GAN-based loss for better sample quality. DMD2 supports both one-step and multi-step generation, and experiments demonstrate its effectiveness in producing high-quality images, surpassing the performance of prior methods, including the original teacher models, with substantially lower inference costs. The findings suggest that DMD2 not only advances the field of fast image synthesis but also holds promise for broader applications in creative industries."}, {"title": "Improving Environment Novelty Quantification for Effective Unsupervised Environment Design", "pdf_url": "https://openreview.net/attachment?id=UdxpjKO2F9&name=pdf", "topics": ["Unsupervised Environment Design", "Novelty Quantification", "Reinforcement Learning", "Curriculum Learning", "Gaussian Mixture Models"], "tldr": "The paper presents the Coverage-based Evaluation of Novelty In Environment (CENIE) framework, which enhances Unsupervised Environment Design (UED) by effectively quantifying environment novelty alongside traditional regret metrics to improve generalization in reinforcement learning agents.", "summary": "This paper addresses the challenges of generalization in reinforcement learning agents by introducing the CENIE framework, which quantifies environment novelty through the lens of state-action space coverage, allowing for more effective curriculum design in UED. CENIE integrates Gaussian Mixture Models to assess novelty dynamically based on the student's accumulated experiences, ultimately complementing existing regret-based methods. Experimental evaluations demonstrate that incorporating CENIE into leading UED algorithms significantly improves zero-shot generalization performance across various benchmarks, indicating the importance of novelty in training robust reinforcement learning agents. This work highlights the potential of combining novelty-driven exploration with traditional regret-based exploitation in developing more capable agents."}, {"title": "Learning diffusion at lightspeed", "pdf_url": "https://openreview.net/attachment?id=y10avdRFNK&name=pdf", "topics": ["diffusion processes", "JKOnet", "energy functional", "machine learning", "potential energy"], "tldr": "JKOnet is a novel method that efficiently learns the energy functionals governing diffusion processes from observational data, significantly outperforming existing models in computational complexity and accuracy.", "summary": "This paper introduces JKOnet, a simplified model designed to learn the energy functionals that drive diffusion processes, such as biological system dynamics and generative model behavior. Unlike existing approaches that rely on complex bilevel optimization, JKOnet leverages first-order necessary optimality conditions, leading to a single-level optimization problem that minimizes a quadratic loss. The authors demonstrate that JKOnet can accurately recover potential, interaction, and internal energy components from population data, achieving state-of-the-art performance with reduced computational costs. Through extensive experiments, including applications to cellular process predictions, the authors show that their method is highly efficient and scalable, thus providing significant insights for modeling complex diffusion dynamics with real-world implications."}, {"title": "Learning Formal Mathematics From Intrinsic Motivation", "pdf_url": "https://openreview.net/attachment?id=uNKlTQ8mBD&name=pdf", "topics": ["Intrinsic Motivation", "Mathematical Reasoning", "Axiomatic Systems", "Theorem Proving", "Learning Agents"], "tldr": "The paper presents MINIMO, an agent that learns to generate and prove mathematical conjectures independently from only the axioms of a formal domain by leveraging intrinsic motivation.", "summary": "This paper introduces MINIMO (Mathematics from Intrinsic Motivation), an artificial agent that collaborates self-improvement in generating challenging yet provable mathematical conjectures and in theorem proving, beginning solely with the axioms of formal mathematical domains. By integrating methods from constrained decoding and type-directed synthesis, MINIMO can generate valid conjectures while simultaneously improving its proof search capabilities through experiences derived from its own failures and successes, a process enhanced via a technique known as hindsight relabeling. Experiments conducted in three axiomatic domains\u2014propositional logic, arithmetic, and group theory\u2014demonstrate that the agent self-improves in both conjecture generation and theorem proving, and that it can effectively tackle previously unused human-generated mathematical theorems. The findings suggest promising avenues for developing autonomous mathematical reasoning systems capable of contributing to formal mathematics."}, {"title": "Learning rigid-body simulators over implicit shapes for large-scale scenes and vision", "pdf_url": "https://openreview.net/attachment?id=QDYts5dYgq&name=pdf", "topics": ["learned simulators", "rigid-body dynamics", "signed-distance functions", "graph networks", "large-scale scenes"], "tldr": "This paper presents SDF-Sim, a learned rigid-body simulator that utilizes signed-distance functions (SDFs) to effectively scale simulations to large scenes with hundreds of objects while maintaining performance.", "summary": "The paper introduces SDF-Sim, a novel approach to simulating rigid-body dynamics in large-scale scenarios by employing learned signed-distance functions (SDFs) for object shape representation. Unlike traditional mesh-based simulators, SDF-Sim overcomes limitations related to collision detection and memory consumption, enabling it to handle scenes with up to 1.1 million nodes. The simulator leverages graph networks to predict object dynamics and demonstrates the ability to extract 3D shapes from multi-view images, thus generalizing to real-world applications. Results indicate SDF-Sim significantly reduces runtime and memory usage compared to state-of-the-art methods, while still achieving realistic simulations of complex interactions among objects. The findings imply strong potential for applications in robotics, animation, and gaming, specifically where efficient real-time simulations are essential."}, {"title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks", "pdf_url": "https://openreview.net/attachment?id=aVh9KRZdRk&name=pdf", "topics": ["in-context learning", "modular arithmetic", "skill composition", "deep learning", "transformers"], "tldr": "The paper investigates the emergence of in-context learning and skill composition in transformer models through a series of modular arithmetic tasks, revealing a transition from memorization to generalization as the number of training tasks increases.", "summary": "This study explores how large language models, specifically transformer-based architectures, develop the capability of in-context learning and skill composition when tasked with linear modular functions. By utilizing a collection of modular arithmetic tasks for pre-training while testing the models' generalization abilities out-of-distribution, the authors demonstrate that as the number of training tasks increases, the models transition from merely memorizing previous examples to leveraging learned skills for generalization. They identify four distinct phases of model performance based on training task diversity and few-shot examples, highlighting a transient nature of generalization in deeper models. The research provides insight into the structured representations formed within the models, contributing to the understanding of emergent capabilities in deep learning systems."}, {"title": "LLM Evaluators Recognize and Favor Their Own Generations", "pdf_url": "https://openreview.net/attachment?id=4NJBV6Wp0h&name=pdf", "topics": ["self-preference", "self-recognition", "large language models", "evaluation bias", "AI safety"], "tldr": "The paper investigates the self-preference bias exhibited by large language models (LLMs), revealing that their ability to recognize their own outputs correlates with a tendency to favor their own generations over those from others.", "summary": "This research explores the phenomenon of self-preference in large language models (LLMs), where these models rate their outputs more favorably than those created by other models or humans. The authors assess whether LLMs can recognize their own texts and how this ability influences their self-preference bias. Through controlled experiments, they demonstrate that models like GPT-4 and Llama 2 show a significant capacity for self-recognition without fine-tuning and that fine-tuning enhances this ability. The results highlight a linear correlation between self-recognition capability and the strength of self-preference bias. The findings have critical implications for AI safety and evaluation methods, particularly for practices relying on self-evaluation, as they could lead to inflated performance assessments."}, {"title": "Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models", "pdf_url": "https://openreview.net/attachment?id=V0oJaLqY4E&name=pdf", "topics": ["maximum entropy", "inverse reinforcement learning", "diffusion models", "energy-based models", "generative modeling"], "tldr": "This paper introduces the Diffusion by Maximum Entropy Inverse Reinforcement Learning (DxMI) framework to enhance the sample quality of diffusion generative models using energy-based models.", "summary": "The authors propose a novel framework called Diffusion by Maximum Entropy Inverse Reinforcement Learning (DxMI), aimed at improving the sample quality of diffusion models, particularly when generating samples in fewer steps. By utilizing an energy-based model (EBM) to represent the log probability density as a reward signal, they establish a joint training procedure for the diffusion model and the EBM. Their approach reformulates the training as a minimax problem, where maximizing entropy facilitates exploration and stabilizes training dynamics. Furthermore, they introduce the Diffusion by Dynamic Programming (DxDP) algorithm, which efficiently updates the diffusion model without backpropagation through time. Empirical results show that diffusion models trained with DxMI generate high-quality samples in as few as 4 to 10 steps while also providing effective training of EBMs for anomaly detection."}, {"title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making", "pdf_url": "https://openreview.net/attachment?id=EKdk4vxKO4&name=pdf", "topics": ["Medical Decision-Making", "Large Language Models", "Multi-Agent Collaboration", "Complexity Assessment", "Adaptive Framework"], "tldr": "The paper presents MDAgents, an adaptive framework utilizing multiple large language models (LLMs) to enhance medical decision-making by dynamically assigning roles based on task complexity, achieving superior performance on diverse medical benchmarks.", "summary": "This research introduces MDAgents, a novel framework designed to leverage collaborations among large language models (LLMs) for effective medical decision-making. The framework mimics real-world clinical processes by automatically assessing the complexity of medical queries and recruiting the appropriate medical expertise as needed, whether that involves a single clinician or a multi-disciplinary team. Through a series of experiments, MDAgents demonstrated superior accuracy compared to traditional solo and group methods across ten medical benchmarks, highlighting significant improvements in handling complex medical tasks requiring multi-modal reasoning. The findings suggest that adaptive collaboration structures can enhance the efficiency and effectiveness of AI-assisted medical diagnoses, encouraging further exploration into integrating LLM capabilities into clinical settings."}, {"title": "MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model", "pdf_url": "https://openreview.net/attachment?id=x7pjdDod6Z&name=pdf", "topics": ["mesh generation", "3D reconstruction", "sparse-view", "transformers", "input guidance"], "tldr": "The paper presents MeshFormer, a novel model for high-quality 3D mesh generation from sparse-view RGB images and normal maps, achieving efficient training and improved detail through a unified single-stage approach with explicit 3D guidance.", "summary": "This paper introduces MeshFormer, an innovative open-world sparse-view reconstruction model that generates high-quality 3D textured meshes in just seconds from a small number of multi-view RGB images and their corresponding normal maps. The model leverages a unique architecture combining 3D voxel representations with transformers, allowing for explicit projective bias and improved learning efficiency. MeshFormer employs a unified single-stage training process by incorporating surface rendering with signed distance function (SDF) supervision, enabling direct high-quality mesh generation. Experimental results demonstrate that MeshFormer outperforms existing methods in terms of mesh quality, requiring significantly less computational resources, and can easily adapt to various 2D diffusion models to facilitate tasks such as single-image-to-3D and text-to-3D generation."}, {"title": "MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map", "pdf_url": "https://openreview.net/attachment?id=Y8YVCOMEpz&name=pdf", "topics": ["linear attention", "softmax approximation", "MetaLA", "transformer models", "optimal design"], "tldr": "This paper introduces MetaLA, a new linear attention mechanism that provides an optimal approximation of softmax attention while meeting criteria for memory efficiency, parameter usage, and model performance.", "summary": "This paper addresses the limitations of existing linear complexity models like LinFormer, State Space Model (SSM), and Linear RNN (LinRNN) in approximating softmax attention within transformer architectures. The authors unify these models under a common framework and derive three necessary conditions for optimal linear attention: dynamic memory capability, static approximation ability, and minimal parameter usage. They propose the Meta Linear Attention (MetaLA) model, which meets these criteria by eliminating unnecessary key matrices, employing self-augmentation techniques, and incorporating short convolutions for enhanced local interactions. Experimental results demonstrate that MetaLA outperforms existing linear attention models across various tasks, including language modeling, associative recall, and image classification, indicating its effectiveness and efficiency as an alternative to traditional softmax attention mechanisms."}, {"title": "Neural Pfaffians: Solving Many Many-Electron Schrdinger Equations", "pdf_url": "https://openreview.net/attachment?id=HRkniCWM3E&name=pdf", "topics": ["Neural Networks", "Quantum Chemistry", "Pfafans", "Many-Electron Systems", "Variational Monte Carlo"], "tldr": "This paper introduces Neural Pfafans, a new class of fully learnable neural wave functions that improve the accuracy and efficiency of approximating the ground states of many-electron systems by utilizing Pfafans instead of traditional Slater determinants.", "summary": "The paper presents Neural Pfafans (NeurPf), a novel framework that addresses the challenges in approximating the ground states of many-electron systems in quantum chemistry. By leveraging Pfafans, the authors circumvent the limitations of Slater determinants, enabling overparametrization without the strict orbital selection constraints related to spin configurations and molecular structures. The empirical evaluation showcases that NeurPf achieves chemical accuracy in calculating ground and ionization energies across various systems, outperforming existing wave function models and traditional reference methods such as CCSD(T) on the TinyMol dataset. Furthermore, the research highlights the model's capability to generalize across diverse chemical systems without substantial loss in accuracy, making it a significant advancement in the computational tools available for quantum chemistry."}, {"title": "NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction", "pdf_url": "https://openreview.net/attachment?id=8qu52Fl1Dt&name=pdf", "topics": ["fMRI", "video reconstruction", "deep learning", "NeuroClips", "semantic perception"], "tldr": "NeuroClips is a novel framework that successfully reconstructs high-fidelity and smooth videos from fMRI data by employing distinct semantics and perception reconstructors to capture both high-level semantics and low-level perceptual details.", "summary": "This paper presents NeuroClips, an innovative framework aimed at reconstructing continuous video sequences from functional magnetic resonance imaging (fMRI) data, which traditionally poses challenges in relating the low temporal resolution of fMRI to the high resolution of video frames. The framework integrates two core components: a Semantics Reconstructor that focuses on accurately capturing high-level semantic features, and a Perception Reconstructor that decodes low-level perceptual details to ensure video smoothness. By employing a pre-trained text-to-video diffusion model, NeuroClips demonstrated significant improvements over existing methods, achieving a 6-second video reconstruction at 8 frames per second with notable enhancements in structural similarity and spatiotemporal metrics. This work bridges the gap between brain activity and video representation, offering new insights into cognitive neuroscience and advancing methodologies in visual stimulus decoding."}, {"title": "Not All Tokens Are What You Need for Pretraining", "pdf_url": "https://openreview.net/attachment?id=0NMzBwqaAJ&name=pdf", "topics": ["Selective Language Modeling", "Token Selection", "Language Model Pretraining", "Data Efficiency", "Mathematical Reasoning"], "tldr": "This paper introduces RHO-1, a language model that employs Selective Language Modeling (SLM) to improve training efficiency by focusing on high-utility tokens, leading to significant performance gains in mathematical reasoning tasks.", "summary": "The paper challenges the conventional approach to language model pretraining that applies uniform loss to all tokens, proposing that not all tokens are equally important for effective model training. It introduces RHO-1, which utilizes Selective Language Modeling (SLM) to selectively train on more useful tokens identified through a reference model. By scoring tokens based on their alignment with desired distributions, RHO-1 demonstrates a remarkable improvement in few-shot accuracy on mathematical tasks after continual pretraining on a reduced dataset. Experimental results show that RHO-1 achieves state-of-the-art performance while leveraging only a fraction of the training data used by previous models, indicating enhanced data efficiency and robustness in the modeling approach for mathematical reasoning tasks."}, {"title": "Optimal Parallelization of Boosting", "pdf_url": "https://openreview.net/attachment?id=rtz4df9IF1&name=pdf", "topics": ["Boosting", "Parallelization", "Weak-to-Strong Learning", "Complexity", "Algorithms"], "tldr": "This paper closes the gap between theoretical lower bounds and practical algorithms for parallelizing boosting methods by introducing an improved algorithm with optimal trade-offs in complexity parameters.", "summary": "The paper explores the parallelization of boosting algorithms, addressing a significant gap between theoretical lower bounds and the performance of existing algorithms. It establishes stronger lower bounds on the parallel complexity of weak-to-strong learners and presents a new algorithm that efficiently uses parallelism to optimize the number of training rounds and parallel work per round. The authors show that their algorithm achieves a trade-off in complexity parameters that matches the established lower bounds, thereby demonstrating nearly sample-optimal performance in the context of parallel boosting. The implications of this research advance the understanding of the inherent trade-offs in boosting and could lead to more efficient implementations in machine learning systems."}, {"title": "Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting", "pdf_url": "https://openreview.net/attachment?id=Ddak3nSqQM&name=pdf", "topics": ["Policy Learning", "Large Language Models", "Reinforcement Learning", "Tutorial Books", "Decision-making"], "tldr": "The paper presents a novel approach called Policy Learning from Tutorial Books (PLfB), which leverages Large Language Models to derive policy networks from written tutorial content, demonstrating significant performance improvements in game environments without real-world interactions.", "summary": "This study introduces the concept of Policy Learning from Tutorial Books (PLfB), which aims to teach agents decision-making skills by extracting knowledge from written materials, notably tutorial books. The authors propose a three-stage framework: Understanding, Rehearsing, and Introspecting (URI) that utilizes Large Language Models (LLMs) to convert text into a structured knowledge database, simulate decision-making processes, and refine policies through introspection of generated datasets. Experiments conducted on Tic-Tac-Toe and Football environments reveal that the URI method consistently outperforms baseline approaches, achieving notable win rates against competing agents without any real data interactions. The findings suggest that leveraging textual knowledge can effectively enhance policy learning for AI agents, opening new avenues for research in offline reinforcement learning."}, {"title": "PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression", "pdf_url": "https://openreview.net/attachment?id=YvA8UF0I37&name=pdf", "topics": ["quantization", "large language models", "fine-tuning", "PV-Tuning", "model compression"], "tldr": "The paper presents PV-Tuning, a novel framework that improves fine-tuning strategies for extreme quantization of large language models (LLMs) to achieve better accuracy and efficiency at 1-2 bits per parameter.", "summary": "This paper addresses the challenges of extreme compression of large language models (LLMs) by proposing PV-Tuning, a framework that enhances the performance of quantization-aware fine-tuning strategies. It critiques existing methods that rely heavily on straight-through estimators for updating compressed weights, revealing their limitations in optimization. By introducing an alternative optimization algorithm that systematically refines both discrete and continuous parameters, PV-Tuning is shown to outperform previous state-of-the-art techniques for quantized models, achieving Pareto-optimal quantization at 2 bits per parameter for Llama-2 models. The experimental results validate the framework's effectiveness using leading LLMs, suggesting significant implications for deploying high-performance models in resource-constrained environments."}, {"title": "Questioning the Survey Responses of Large Language Models", "pdf_url": "https://openreview.net/attachment?id=Oo7dlLgqQX&name=pdf", "topics": ["large language models", "survey responses", "biases", "American Community Survey", "alignment metrics"], "tldr": "This paper critically analyzes the use of survey methodologies to evaluate large language models, revealing significant biases in their responses that undermine their resemblance to human populations.", "summary": "The paper investigates the efficacy of utilizing survey responses from large language models (LLMs) to understand their demographic and alignment characteristics, using questions from the American Community Survey (ACS) as a basis. Through systematic prompting of 43 different language models, the authors identify substantial ordering and labeling biases affecting the models' responses, which often appear closer to uniform distributions regardless of the size or pre-training data of the models. The study concludes that even after adjusting for these biases, LLMs do not meaningfully align with U.S. census data, which raises concerns about the validity of conclusions drawn from survey-derived alignment metrics. These findings suggest that current methodologies may yield misleading insights regarding the representation of various demographic subgroups by LLMs, calling for caution in applying survey results to model evaluation and a need for further research on effective assessment strategies."}, {"title": "Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity", "pdf_url": "https://openreview.net/attachment?id=qf2uZAdy1N&name=pdf", "topics": ["reinforcement learning", "latent dynamics", "statistical modularity", "algorithmic modularity", "representation learning"], "tldr": "This paper investigates the challenges and solutions of reinforcement learning in environments with complex observations but simple latent dynamics, establishing a framework for statistical and algorithmic modularity.", "summary": "The paper addresses the complexities associated with reinforcement learning (RL) in environments characterized by rich, high-dimensional observations while underlying dynamics are governed by a simpler, unobserved latent state space. It introduces a theoretical framework that explores two major aspects: statistical modularity, which relates to the tractability of learning under latent dynamics based on existing RL settings, and algorithmic modularity, which enables the efficient adaptation of latent RL algorithms to observable settings. The authors present a negative result showing that many well-studied RL settings do not maintain statistical modularity under latent dynamics and complement this with positive results demonstrating that specific structural conditions, like pushforward coverability, can enable practical learning. Additionally, the paper develops algorithms that transform existing RL methods for latent dynamics into observable settings, showcasing the foundational work towards a unified theory in this domain."}, {"title": "Return of Unconditional Generation: A Self-supervised Representation Generation Method", "pdf_url": "https://openreview.net/attachment?id=clTa4JFBML&name=pdf", "topics": ["Unconditional Generation", "Self-Supervised Learning", "Representation Conditioned Generation", "Image Synthesis", "Generative Models"], "tldr": "This paper presents a novel framework called Representation-Conditioned Generation (RCG) that significantly enhances the quality of unconditional image generation by leveraging self-supervised representations instead of human labels.", "summary": "The paper addresses the long-standing challenge of unconditional image generation using a new framework, Representation-Conditioned Generation (RCG). By mapping images to a low-dimensional representation space via a self-supervised encoder, RCG generates abstract representations that serve as conditioning inputs for an image generator, thus bridging the performance gap with conditional generation methods. Comprehensive experiments demonstrate that RCG achieves a state-of-the-art Frechet Inception Distance (FID) of 2.15 on the ImageNet dataset, greatly outperforming previous unconditional methods and approaching the quality of leading conditional methods. This framework effectively utilizes the vast amounts of available unlabeled data, offering a promising direction for future research in generative models."}, {"title": "RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation", "pdf_url": "https://openreview.net/attachment?id=r5spnrY6H3&name=pdf", "topics": ["3D Referring Expression Segmentation", "Spatial Awareness", "Rule-Guided Weak Supervision", "Text-driven Localization Module", "Point Clouds"], "tldr": "The paper introduces RG-SAN, a Rule-Guided Spatial Awareness Network that enhances 3D referring expression segmentation by effectively modeling spatial relationships and using weak supervision for improved accuracy.", "summary": "This paper presents the Rule-Guided Spatial Awareness Network (RG-SAN) to address the challenges of 3D referring expression segmentation (3D-RES). Traditional methods struggle with over-segmentation and mis-segmentation due to a lack of focus on spatial information among instances. RG-SAN comprises a Text-driven Localization Module (TLM) that accurately locates mentioned instances and iteratively refines their positions, alongside a Rule-guided Weak Supervision (RWS) strategy that leverages dependency tree rules for positional guidance based solely on the target instance. Experimental results on the ScanRefer benchmark demonstrate that RG-SAN achieves a mean Intersection over Union (mIoU) improvement of 5.1 points compared to prior models and shows enhanced robustness against spatial ambiguities in descriptions, emphasizing the significance of incorporating spatial context in visual grounding tasks."}, {"title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy", "pdf_url": "https://openreview.net/attachment?id=LEzx6QRkRH&name=pdf", "topics": ["Reinforcement Learning", "Code-as-Policy", "Large Language Models", "Hierarchical Framework", "Minecraft"], "tldr": "The paper introduces RL-GPT, a novel framework that integrates reinforcement learning and large language models to enhance task learning in complex environments, achieving superior performance in the Minecraft game.", "summary": "This paper presents RL-GPT, a two-level hierarchical framework designed to merge reinforcement learning (RL) with large language models (LLMs) for effective task learning in open-world environments like Minecraft. The approach features a slow agent that decomposes tasks into manageable sub-actions and decides which can be implemented through coding, while a fast agent executes these coded actions and fine-tunes them through RL. Experimental results demonstrate that RL-GPT significantly outperforms traditional RL methods and existing LLM-based agents, achieving high success rates in various tasks within Minecraft, including complex long-horizon challenges like obtaining diamonds. The integration of high-level planning and low-level action control is posited to improve sample efficiency and adaptability in task execution."}, {"title": "Scale Equivariant Graph Metanetworks", "pdf_url": "https://openreview.net/attachment?id=8Fxqn1tZM1&name=pdf", "topics": ["Scaling symmetries", "MetaNetworks", "Neural networks", "Equivariance", "Graph structures"], "tldr": "The paper proposes Scale Equivariant Graph MetaNetworks (ScaleGMNs), which integrate scaling symmetries into the representation and processing of neural networks for improved performance across various architectures and datasets.", "summary": "This paper introduces Scale Equivariant Graph MetaNetworks (ScaleGMNs), which extend existing Graph Metanetworks by incorporating scaling symmetries present in neural networks, particularly those arising from typical activation functions. The authors establish the design principles of ScaleGMNs, demonstrating their ability to maintain equivariance to both permutations and scaling operations during the processing of feedforward neural networks. They provide theoretical guarantees on the expressivity of ScaleGMNs, showing that they can simulate both the forward and backward passes of any input feedforward neural network. Experimental results indicate that ScaleGMNs outperform state-of-the-art methods across multiple datasets and activation functions, highlighting the significance of scaling symmetries as a vital inductive bias in neural network processing."}, {"title": "SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling", "pdf_url": "https://openreview.net/attachment?id=mSaqxZVZW8&name=pdf", "topics": ["A* search", "Monte-Carlo tree search", "heuristic search", "selective sampling", "exploration"], "tldr": "This paper introduces SeeA, an enhanced A* search algorithm that leverages selective sampling to improve exploration and efficiency when heuristic accuracy is insufficient.", "summary": "The paper presents SeeA (Sampling-exploration enhanced A* search), a novel search algorithm aimed at improving the efficiency of classical A* search by incorporating a selective sampling method to construct a dynamic candidate subset of open nodes. Unlike traditional A*, which consistently expands the node with the best heuristic value, SeeA diversifies exploration by sometimes selecting nodes that may not have the best heuristic estimates, thus enabling the discovery of more promising branches in the search space. The research also investigates three sampling techniques\u2014uniform, clustering, and UCT-like sampling\u2014demonstrating SeeA's theoretical superiority over standard A* when heuristic functions are inaccurate. Empirical results from applications in retrosynthetic planning, logic synthesis, and the Sokoban game indicate that SeeA significantly outperforms leading heuristic search algorithms in problem-solving success rates and solution quality while minimizing node expansions."}, {"title": "Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs", "pdf_url": "https://openreview.net/attachment?id=pGEY8JQ3qx&name=pdf", "topics": ["sample complexity", "average-reward MDPs", "discounting", "weakly communicating", "generative models"], "tldr": "The paper establishes optimal sample complexity bounds for learning near-optimal policies in weakly communicating and general average-reward Markov decision processes (MDPs).", "summary": "This paper investigates the sample complexity required to learn \u03b5-optimal policies in average-reward MDPs using a generative model. The authors derive sample complexity bounds for weakly communicating MDPs, demonstrating that eO(SAH\u00b2/\u03b5) samples suffice, where H represents the span of the bias function of the optimal policy. They also introduce a transient time parameter B for general average-reward MDPs, establishing a sample complexity of eO(SAB + H\u00b2/\u03b5) and proving a matching lower bound. The results highlight the necessity of considering both span and transient time parameters for optimal sample complexity in reinforcement learning scenarios, improving upon previously known bounds by minimizing dependence on the effective horizon for both specific and general MDP instances."}, {"title": "Statistical Efficiency of Distributional Temporal Difference Learning", "pdf_url": "https://openreview.net/attachment?id=eWUM5hRYgH&name=pdf", "topics": ["Distributional reinforcement learning", "distributional temporal difference learning", "statistical efficiency", "convergence bounds", "Wasserstein distance."], "tldr": "The paper analyzes the finite-sample performance of distributional temporal difference learning algorithms, establishing minimax optimal sample complexity bounds for both non-parametric and categorical instances.", "summary": "This paper investigates the statistical efficiency of distributional temporal difference (TD) learning in the context of distributional reinforcement learning (DRL), primarily focusing on finite-sample performance. It introduces non-parametric distributional TD (NTD) and revisits categorical TD (CTD), demonstrating that both require approximately \\(O(\\frac{1}{2p(1)^{2p+1}})\\) iterations to achieve -optimal estimators in the \\(p\\)-Wasserstein distance. The results indicate that these bounds are minimax optimal, contributing significantly to our understanding of distributional policy evaluation in reinforcement learning. Additionally, the authors establish a novel Freedman's inequality in Hilbert spaces, which provides a theoretical tool of independent interest for future research."}, {"title": "Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators", "pdf_url": "https://openreview.net/attachment?id=J2wI2rCG2u&name=pdf", "topics": ["Stochastic Taylor Derivative Estimator", "High-Dimensional PDEs", "Physics-Informed Neural Networks", "Automatic Differentiation", "Computational Efficiency"], "tldr": "The paper presents the Stochastic Taylor Derivative Estimator (STDE), which significantly improves the computation of arbitrary high-order differential operators in neural networks, achieving over 1000x speed-up and 30% memory reduction over traditional methods.", "summary": "This paper introduces the Stochastic Taylor Derivative Estimator (STDE) to efficiently compute high-dimensional and high-order derivatives in neural networks, particularly within the context of Physics-Informed Neural Networks (PINNs). The authors address the computational challenges associated with the exponential growth of derivative tensor size and complexity by utilizing high-order automatic differentiation combined with randomization techniques. Their method demonstrates remarkable performance improvements, including substantial reductions in evaluation time and memory requirements, allowing for the solution of million-dimensional partial differential equations within minutes on standard GPUs. STDE not only generalizes previous approaches but also enhances the feasibility of employing complex differential operators in large-scale machine learning tasks, thereby advancing the capabilities of PINNs in practical applications."}, {"title": "Stylus: Automatic Adapter Selection for Diffusion Models", "pdf_url": "https://openreview.net/attachment?id=3Odq2tGSpp&name=pdf", "topics": ["adapter selection", "image generation", "diffusion models", "Stable Diffusion", "visual fidelity"], "tldr": "The paper introduces Stylus, a system that automatically selects and composes highly relevant adapters for diffusion models, enhancing image generation quality and diversity based on user prompts.", "summary": "The paper presents Stylus, a novel method for improving image generation using diffusion models by automatically selecting and composing low-rank adaptation (LoRA) adapters based on user-provided prompts. Stylus employs a three-stage framework consisting of a refiner that generates better descriptions of adapter tasks, a retriever that fetches relevant adapters, and a composer that segments prompts into distinct tasks and assigns appropriate adapters to them. The authors developed StylusDocs, a curated dataset containing 75,000 adapters with enhanced documentation, to evaluate Stylus's effectiveness. Results indicate that Stylus achieves superior visual fidelity, textual alignment, and image diversity over existing models, demonstrating its potential to enhance generative AI art."}, {"title": "The Road Less Scheduled", "pdf_url": "https://openreview.net/attachment?id=0XeNkkENuI&name=pdf", "topics": ["Schedule-Free Optimization", "Learning Rate Schedules", "Gradient Descent", "Stochastic Optimization", "Empirical Evaluation"], "tldr": "The paper introduces a novel Schedule-Free optimization method that outperforms existing learning rate schedules in various machine learning tasks without the need for specifying a stopping time.", "summary": "This paper proposes a Schedule-Free optimization approach that eliminates the necessity for learning rate schedules, which are typically dependent on a predetermined optimization stopping time. The method unifies theoretical insights from scheduling and iterate averaging, resulting in state-of-the-art performance across different optimization problems, including deep learning and convex settings. Utilizing a modified momentum approach, it circumvents the need for additional hyper-parameters, demonstrating superior or comparable results to optimized cosine schedules in extensive empirical evaluations. The success of Schedule-Free AdamW is validated through its performance in the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge, showcasing its practical applicability and effectiveness in real-world tasks."}, {"title": "The Sample-Communication Complexity Trade-off in Federated Q-Learning", "pdf_url": "https://openreview.net/attachment?id=6YIpvnkjUK&name=pdf", "topics": ["Federated Q-learning", "communication complexity", "sample complexity", "algorithm design", "reinforcement learning."], "tldr": "This paper presents a comprehensive study of the sample-communication complexity trade-off in Federated Q-learning, introducing a novel algorithm, Fed-DVR-Q, that achieves optimal performance in both metrics simultaneously.", "summary": "The paper investigates the collaborative learning of optimal Q-functions in an infinite horizon Markov Decision Process through Federated Q-learning, focusing on the trade-off between sample complexity and communication complexity. It establishes lower bounds on communication costs necessary for achieving collaborative benefits, revealing that any algorithm offering linear sample efficiency must incur significant communication costs. The authors then introduce Fed-DVR-Q, the first Federated Q-learning algorithm that achieves both order-optimal sample complexity and minimal communication costs, thereby fully characterizing the sample-communication complexity landscape for this domain. Through theoretical results and numerical experiments, the findings highlight the effectiveness and efficiency of the developed approach, which provides valuable insights for the design of practical federated learning systems."}, {"title": "Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes", "pdf_url": "https://openreview.net/attachment?id=REIK4SZMJt&name=pdf", "topics": ["hippocampus", "place cells", "contextual capacity", "memory encoding", "neural geometry"], "tldr": "The paper demonstrates that the contextual capacity of hippocampal place cell codes grows exponentially with the number of neurons, revealing a trade-off between spatial resolution and context discrimination.", "summary": "This paper investigates how hippocampal place cells, which are key to cognitive mapping in animals, encode contextual information by analyzing their firing properties through a geometric framework. The authors present a model showing that the number of distinct contexts the hippocampus can store increases exponentially with the number of place cells, while identifying a fundamental trade-off between high spatial resolution and the ability to distinguish multiple contexts. Furthermore, they explore how the physical arrangement of place cells, particularly clustering near boundaries, enhances contextual discrimination. The findings suggest that variability in place cell widths across the hippocampus may be an adaptive feature, optimizing the encoding of spatial and contextual information according to different behavioral needs."}, {"title": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought", "pdf_url": "https://openreview.net/attachment?id=pC44UMwy2v&name=pdf", "topics": ["Chain-of-Thought (CoT)", "Reasoning Boundary Framework", "Large Language Models (LLMs)", "Optimization Strategies", "Quantification Metrics"], "tldr": "This paper introduces a novel Reasoning Boundary Framework (RBF) to quantify and optimize Chain-of-Thought reasoning in Large Language Models, establishing a systematic approach to enhance model performance across various complex tasks.", "summary": "The paper presents a Reasoning Boundary Framework (RBF) to address the challenges of quantifying and optimizing Chain-of-Thought (CoT) capabilities in Large Language Models (LLMs). It defines the concept of a Reasoning Boundary (RB) to measure the upper limits of task-specific reasoning complexity and formulates a combination law for RB to enable practical quantification across diverse CoT tasks. The authors categorize RBs into three types\u2014Completely Feasible, Partially Feasible, and Completely Infeasible\u2014while proposing optimization techniques based on reasoning path adjustments and RB promotion strategies. Through extensive experiments involving 27 models across five complex reasoning tasks, the framework's effectiveness is validated, providing valuable insights into CoT strategies and laying the groundwork for future advancements in LLM optimization."}, {"title": "VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time", "pdf_url": "https://openreview.net/attachment?id=5zSCSE0k41&name=pdf", "topics": ["audio-driven generation", "talking faces", "facial dynamics", "real-time rendering", "artificial intelligence"], "tldr": "The paper presents V ASA-1, a framework that generates lifelike talking faces in real-time from a single image and audio, achieving high realism and expressiveness in facial dynamics.", "summary": "This paper introduces V ASA-1, an advanced framework designed to create realistic talking face videos from a static image and a speech audio clip. The model leverages a diffusion-based approach to generate facial movements and head dynamics in a holistic latent space, significantly improving synchronization with audio and the expressiveness of facial nuances compared to prior methods. Extensive experiments demonstrate that V ASA-1 outperforms existing approaches across various metrics, achieving high video quality and real-time performance. The implications of this work extend to enhancing virtual human interactions across different applications, including education and healthcare, while addressing the challenges of latency and realism in digital communication."}, {"title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction", "pdf_url": "https://openreview.net/attachment?id=gojL67CfS8&name=pdf", "topics": ["Visual Autoregressive Modeling", "Image Generation", "Scaling Laws", "Transformer Architecture", "Zero-Shot Generalization"], "tldr": "This paper introduces Visual AutoRegressive (V AR) modeling, a new image generation framework that significantly outperforms existing autoregressive and diffusion models by utilizing a coarse-to-fine next-scale prediction method.", "summary": "The paper presents Visual AutoRegressive (V AR) modeling, which redefines autoregressive learning in image generation to focus on next-scale prediction rather than traditional next-token prediction. This method enables fast learning of visual distributions and improves performance on the ImageNet benchmark, achieving a Fr\u00e9chet Inception Distance (FID) of 1.73 and an Inception Score (IS) of 350.2, along with twenty-fold faster inference speeds compared to baseline autoregressive models. V AR exhibits strong scalability, demonstrated by power-law relationships between model size, training compute, and performance metrics, alongside promising zero-shot generalization capabilities in image inpainting and editing tasks. The findings suggest V AR effectively harnesses strengths from large language models and enhances generative capabilities in vision."}, {"title": "Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning", "pdf_url": "https://openreview.net/attachment?id=9O2sVnEHor&name=pdf", "topics": ["graph neural networks", "Weisfeiler-Leman", "expressive power", "cycles", "homomorphism counting"], "tldr": "This paper introduces the r-loopy Weisfeiler-Leman (r-`WL) algorithm and the corresponding graph neural network framework (r-`MPNN), which effectively counts cycles of varying lengths and enables distinguishing complex graph structures beyond the limitations of existing methods.", "summary": "The paper presents r-loopy Weisfeiler-Leman (r-`WL), a novel hierarchy of graph isomorphism tests aimed at improving the expressive power of graph neural networks (GNNs). The authors propose the r-`MPNN framework, capable of counting cycles up to length r+2, thus overcoming limitations faced by traditional GNNs, specifically those that rely on the Weisfeiler-Leman (WL) algorithm. They demonstrate that r-`WL can count homomorphisms of cactus graphs, which is a significant enhancement over existing approaches like k-WL. Empirical validation on both synthetic and real-world datasets shows that r-`MPNN scales efficiently and outperforms current state-of-the-art models, especially on sparse graphs, highlighting its potential for applications in various fields that require intricate graph structure analysis."}, {"title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models", "pdf_url": "https://openreview.net/attachment?id=25Ioxw576r&name=pdf", "topics": ["decoder-decoder architecture", "YOCO", "key-value caching", "long-context modeling", "efficiency"], "tldr": "The paper presents YOCO, a novel decoder-decoder architecture for large language models that significantly reduces GPU memory usage and prefill latency while maintaining strong performance for long-context tasks.", "summary": "This paper introduces YOCO, a decoder-decoder architecture designed for large language models that optimizes memory usage by only caching key-value (KV) pairs once, employing a self-decoder to generate global KV caches and a cross-decoder to reuse them. This approach allows YOCO to effectively handle long-context input, achieving scalability up to 1 million tokens and demonstrating improved inference performance compared to traditional transformers. Experimental evaluations show YOCO's competitive language modeling capabilities, with remarkable enhancements in prefill latency and GPU memory consumption, positioning it as a robust solution for future large language models built for long-sequence tasks."}];
        let currentPage = 1;
        
        // Function to filter papers based on search input
        function filterPapers(searchText) {
            if (!searchText) return papersData;
            
            searchText = searchText.toLowerCase();
            return papersData.filter(paper => {
                const titleMatch = paper.title.toLowerCase().includes(searchText);
                const topicsMatch = paper.topics.some(topic => 
                    topic.toLowerCase().includes(searchText)
                );
                const tldrMatch = paper.tldr.toLowerCase().includes(searchText);
                const summaryMatch = paper.summary.toLowerCase().includes(searchText);
                
                return titleMatch || topicsMatch || tldrMatch || summaryMatch;
            });
        }
        
        // Function to create paper card HTML
        function createPaperCard(paper) {
            const showTopics = document.getElementById('showTopics').checked;
            const showTldr = document.getElementById('showTldr').checked;
            const showSummary = document.getElementById('showSummary').checked;

            const topicsHtml = (showTopics && paper.topics.length > 0)
                ? `<div class="mb-3">
                     <div class="d-flex gap-2 flex-wrap">
                       ${paper.topics.map(topic => `<span class="badge text-bg-info">${topic}</span>`).join('')}
                     </div>
                   </div>`
                : '';
                
            const tldrHtml = (showTldr && paper.tldr)
                ? `<div class="mb-3">
                     <h3 class="h5">TL;DR</h3>
                     <p class="card-text">${paper.tldr}</p>
                   </div>`
                : '';
                
            const summaryHtml = (showSummary && paper.summary)
                ? `<div class="mb-3">
                     <h3 class="h5">Summary</h3>
                     <p class="card-text">${paper.summary}</p>
                   </div>`
                : '';
                
            const urlHtml = paper.pdf_url
                ? `<p class="card-text"><a href="${paper.pdf_url}" class="btn btn-outline-primary btn-sm">Download Paper</a></p>`
                : '';
                
            return `
                <div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
                    <div class="card shadow-sm">
                        <div class="card-body">
                            <h3 class="card-title h4">${paper.title}</h3>
                            ${topicsHtml}
                            ${tldrHtml}
                            ${summaryHtml}
                            ${urlHtml}
                        </div>
                    </div>
                </div>
            `;
        }

        // Function to create pagination controls
        function createPagination(totalItems) {
            const itemsPerPage = parseInt(document.getElementById('itemsPerPage').value);
            const totalPages = Math.ceil(totalItems / itemsPerPage);
            const pagination = document.getElementById('pagination');
            
            let paginationHtml = '';
            
            // Previous button
            paginationHtml += `
                <li class="page-item ${currentPage === 1 ? 'disabled' : ''}">
                    <a class="page-link" href="#" data-page="${currentPage - 1}">Previous</a>
                </li>
            `;
            
            // Page numbers with ellipsis
            const maxVisiblePages = 5;  // Adjust this number to show more or fewer pages
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust startPage if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            // First page and ellipsis
            if (startPage > 1) {
                paginationHtml += `
                    <li class="page-item">
                        <a class="page-link" href="#" data-page="1">1</a>
                    </li>
                `;
                if (startPage > 2) {
                    paginationHtml += `
                        <li class="page-item disabled">
                            <span class="page-link">...</span>
                        </li>
                    `;
                }
            }
            
            // Visible pages
            for (let i = startPage; i <= endPage; i++) {
                paginationHtml += `
                    <li class="page-item ${currentPage === i ? 'active' : ''}">
                        <a class="page-link" href="#" data-page="${i}">${i}</a>
                    </li>
                `;
            }
            
            // Last page and ellipsis
            if (endPage < totalPages) {
                if (endPage < totalPages - 1) {
                    paginationHtml += `
                        <li class="page-item disabled">
                            <span class="page-link">...</span>
                        </li>
                    `;
                }
                paginationHtml += `
                    <li class="page-item">
                        <a class="page-link" href="#" data-page="${totalPages}">${totalPages}</a>
                    </li>
                `;
            }
            
            // Next button
            paginationHtml += `
                <li class="page-item ${currentPage === totalPages ? 'disabled' : ''}">
                    <a class="page-link" href="#" data-page="${currentPage + 1}">Next</a>
                </li>
            `;
            
            pagination.innerHTML = paginationHtml;
            
            // Add click handlers
            pagination.querySelectorAll('.page-link').forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const newPage = parseInt(e.target.dataset.page);
                    if (!isNaN(newPage) && newPage >= 1 && newPage <= totalPages) {
                        currentPage = newPage;
                        renderPapers();
                    }
                });
            });
        }

        // Render papers with pagination
        function renderPapers() {
            const searchText = document.getElementById('searchInput').value;
            const filteredPapers = filterPapers(searchText);
            const itemsPerPage = parseInt(document.getElementById('itemsPerPage').value);
            
            // Calculate pagination
            const startIndex = (currentPage - 1) * itemsPerPage;
            const endIndex = startIndex + itemsPerPage;
            const paginatedPapers = filteredPapers.slice(startIndex, endIndex);
            
            // Render papers
            const container = document.getElementById('papers-container');
            container.innerHTML = paginatedPapers.map(createPaperCard).join('');
            
            // Create pagination controls
            createPagination(filteredPapers.length);
            
            // Initialize Masonry layout
            new Masonry(container, {
                percentPosition: true
            });
        }

        // Add event listeners
        document.getElementById('searchInput').addEventListener('input', () => {
            currentPage = 1;  // Reset to first page on search
            renderPapers();
        });
        document.getElementById('itemsPerPage').addEventListener('change', () => {
            currentPage = 1;  // Reset to first page when changing items per page
            renderPapers();
        });
        document.getElementById('showTopics').addEventListener('change', renderPapers);
        document.getElementById('showTldr').addEventListener('change', renderPapers);
        document.getElementById('showSummary').addEventListener('change', renderPapers);

        // Render papers when page loads
        document.addEventListener('DOMContentLoaded', renderPapers);
    </script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
